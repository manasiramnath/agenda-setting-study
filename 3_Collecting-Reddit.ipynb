{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f418aa-fc77-42f2-9aa5-c2697c352727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import logging.handlers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b09a1f2-c7c1-4687-bdd8-baf422cbb210",
   "metadata": {},
   "source": [
    "## Decompressing files\n",
    "\n",
    "Code from user Watchful1: https://github.com/Watchful1/PushshiftDumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the path to the input file, or a folder of files to process all of\n",
    "input_folder = r\"/Volumes/Untitled/reddit/subreddits23\"\n",
    "# put the name or path to the output file. The file extension from below will be added automatically. If the input file is a folder, the output will be treated as a folder as well\n",
    "output_folder = r\"/Volumes/Untitled/reddit/subreddits23_csv\"\n",
    "# the format to output in, pick from the following options\n",
    "#   zst: same as the input, a zstandard compressed ndjson file. Can be read by the other scripts in the repo\n",
    "#   txt: an ndjson file, which is a text file with a separate json object on each line. Can be opened by any text editor\n",
    "#   csv: a comma separated value file. Can be opened by a text editor or excel\n",
    "# WARNING READ THIS: if you use txt or csv output on a large input file without filtering out most of the rows, the resulting file will be extremely large. Usually about 7 times as large as the compressed input file\n",
    "output_format = \"csv\"\n",
    "# override the above format and output only this field into a text file, one per line. Useful if you want to make a list of authors or ids. See the examples below\n",
    "# any field that's in the dump is supported, but useful ones are\n",
    "#   author: the username of the author\n",
    "#   id: the id of the submission or comment\n",
    "#   link_id: only for comments, the fullname of the submission the comment is associated with\n",
    "#   parent_id: only for comments, the fullname of the parent of the comment. Either another comment or the submission if it's top level\n",
    "single_field = None\n",
    "# the fields in the file are different depending on whether it has comments or submissions. If we're writing a csv, we need to know which fields to write.\n",
    "# set this to true to write out to the log every time there's a bad line, set to false if you're expecting only some of the lines to match the key\n",
    "write_bad_lines = True\n",
    "\n",
    "# only output items between these two dates\n",
    "from_date = datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\")\n",
    "to_date = datetime.strptime(\"2023-12-31\", \"%Y-%m-%d\")\n",
    "\n",
    "field = \"body\"\n",
    "values = ['']\n",
    "# if you have a long list of values, you can put them in a file and put the filename here. If set this overrides the value list above\n",
    "# if this list is very large, it could greatly slow down the process\n",
    "values_file = None\n",
    "exact_match = False\n",
    "\n",
    "# sets up logging to the console as well as a file\n",
    "log = logging.getLogger(\"bot\")\n",
    "log.setLevel(logging.INFO)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')\n",
    "log_str_handler = logging.StreamHandler()\n",
    "log_str_handler.setFormatter(log_formatter)\n",
    "log.addHandler(log_str_handler)\n",
    "if not os.path.exists(\"logs\"):\n",
    "\tos.makedirs(\"logs\")\n",
    "log_file_handler = logging.handlers.RotatingFileHandler(os.path.join(\"logs\", \"bot.log\"), maxBytes=1024*1024*16, backupCount=5)\n",
    "log_file_handler.setFormatter(log_formatter)\n",
    "log.addHandler(log_file_handler)\n",
    "\n",
    "\n",
    "def write_line_zst(handle, line):\n",
    "\thandle.write(line.encode('utf-8'))\n",
    "\thandle.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "\n",
    "def write_line_json(handle, obj):\n",
    "\thandle.write(json.dumps(obj))\n",
    "\thandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "def write_line_single(handle, obj, field):\n",
    "\tif field in obj:\n",
    "\t\thandle.write(obj[field])\n",
    "\telse:\n",
    "\t\tlog.info(f\"{field} not in object {obj['id']}\")\n",
    "\thandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "def write_line_csv(writer, obj, is_submission):\n",
    "    output_list = []\n",
    "    output_list.append(str(obj['score']))\n",
    "    output_list.append(datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d\"))\n",
    "    if is_submission:\n",
    "        output_list.append(obj['title'])\n",
    "        output_list.append(obj.get('num_comments', 0))  # Include num_comments for submissions\n",
    "    else:\n",
    "        output_list.append(obj.get('is_submitter', False))  # Include is_submitter for comments\n",
    "    output_list.append(f\"u/{obj['author']}\")\n",
    "    output_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "    if is_submission:\n",
    "        if obj['is_self']:\n",
    "            if 'selftext' in obj:\n",
    "                output_list.append(obj['selftext'])\n",
    "            else:\n",
    "                output_list.append(\"\")\n",
    "        else:\n",
    "            output_list.append(obj['url'])\n",
    "    else:\n",
    "        output_list.append(obj['body'])\n",
    "    writer.writerow(output_list)\n",
    "\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "\tchunk = reader.read(chunk_size)\n",
    "\tbytes_read += chunk_size\n",
    "\tif previous_chunk is not None:\n",
    "\t\tchunk = previous_chunk + chunk\n",
    "\ttry:\n",
    "\t\treturn chunk.decode()\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\tif bytes_read > max_window_size:\n",
    "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n",
    "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "\twith open(file_name, 'rb') as file_handle:\n",
    "\t\tbuffer = ''\n",
    "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "\t\twhile True:\n",
    "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "\t\t\tif not chunk:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "\t\t\tfor line in lines[:-1]:\n",
    "\t\t\t\tyield line.strip(), file_handle.tell()\n",
    "\n",
    "\t\t\tbuffer = lines[-1]\n",
    "\n",
    "\t\treader.close()\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file, output_format, field, values, from_date, to_date, single_field, exact_match):\n",
    "\toutput_path = f\"{output_file}.{output_format}\"\n",
    "\tis_submission = \"submission\" in input_file\n",
    "\tlog.info(f\"Input: {input_file} : Output: {output_path} : Is submission {is_submission}\")\n",
    "\twriter = None\n",
    "\tif output_format == \"zst\":\n",
    "\t\thandle = zstandard.ZstdCompressor().stream_writer(open(output_path, 'wb'))\n",
    "\telif output_format == \"txt\":\n",
    "\t\thandle = open(output_path, 'w', encoding='UTF-8')\n",
    "\telif output_format == \"csv\":\n",
    "\t\thandle = open(output_path, 'w', encoding='UTF-8', newline='')\n",
    "\t\twriter = csv.writer(handle)\n",
    "\telse:\n",
    "\t\tlog.error(f\"Unsupported output format {output_format}\")\n",
    "\t\tsys.exit()\n",
    "\n",
    "\tfile_size = os.stat(input_file).st_size\n",
    "\tcreated = None\n",
    "\tmatched_lines = 0\n",
    "\tbad_lines = 0\n",
    "\ttotal_lines = 0\n",
    "\tfor line, file_bytes_processed in read_lines_zst(input_file):\n",
    "\t\ttotal_lines += 1\n",
    "\t\tif total_lines % 100000 == 0:\n",
    "\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {total_lines:,} : {matched_lines:,} : {bad_lines:,} : {file_bytes_processed:,}:{(file_bytes_processed / file_size) * 100:.0f}%\")\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tobj = json.loads(line)\n",
    "\t\t\tcreated = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
    "\n",
    "\t\t\tif created < from_date:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif created > to_date:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif field is not None:\n",
    "\t\t\t\tfield_value = obj[field].lower()\n",
    "\t\t\t\tmatched = False\n",
    "\t\t\t\tfor value in values:\n",
    "\t\t\t\t\tif exact_match:\n",
    "\t\t\t\t\t\tif value == field_value:\n",
    "\t\t\t\t\t\t\tmatched = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif value in field_value:\n",
    "\t\t\t\t\t\t\tmatched = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif not matched:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tmatched_lines += 1\n",
    "\t\t\tif output_format == \"zst\":\n",
    "\t\t\t\twrite_line_zst(handle, line)\n",
    "\t\t\telif output_format == \"csv\":\n",
    "\t\t\t\twrite_line_csv(writer, obj, is_submission)\n",
    "\t\t\telif output_format == \"txt\":\n",
    "\t\t\t\tif single_field is not None:\n",
    "\t\t\t\t\twrite_line_single(handle, obj, single_field)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twrite_line_json(handle, obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlog.info(f\"Something went wrong, invalid output format {output_format}\")\n",
    "\t\texcept (KeyError, json.JSONDecodeError) as err:\n",
    "\t\t\tbad_lines += 1\n",
    "\t\t\tif write_bad_lines:\n",
    "\t\t\t\tif isinstance(err, KeyError):\n",
    "\t\t\t\t\tlog.warning(f\"Key {field} is not in the object: {err}\")\n",
    "\t\t\t\telif isinstance(err, json.JSONDecodeError):\n",
    "\t\t\t\t\tlog.warning(f\"Line decoding failed: {err}\")\n",
    "\t\t\t\tlog.warning(line)\n",
    "\n",
    "\thandle.close()\n",
    "\tlog.info(f\"Complete : {total_lines:,} : {matched_lines:,} : {bad_lines:,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tif single_field is not None:\n",
    "\t\tlog.info(\"Single field output mode, changing output file format to txt\")\n",
    "\t\toutput_format = \"txt\"\n",
    "\n",
    "\tif values_file is not None:\n",
    "\t\tvalues = []\n",
    "\t\twith open(values_file, 'r') as values_handle:\n",
    "\t\t\tfor value in values_handle:\n",
    "\t\t\t\tvalues.append(value.strip().lower())\n",
    "\t\tlog.info(f\"Loaded {len(values)} from values file {values_file}\")\n",
    "\telse:\n",
    "\t\tvalues = [value.lower() for value in values]  # convert to lowercase\n",
    "\n",
    "\tlog.info(f\"Filtering field: {field}\")\n",
    "\tif len(values) <= 20:\n",
    "\t\tlog.info(f\"On values: {','.join(values)}\")\n",
    "\telse:\n",
    "\t\tlog.info(f\"On values:\")\n",
    "\t\tfor value in values:\n",
    "\t\t\tlog.info(value)\n",
    "\tlog.info(f\"Exact match {('on' if exact_match else 'off')}. Single field {single_field}.\")\n",
    "\tlog.info(f\"From date {from_date.strftime('%Y-%m-%d')} to date {to_date.strftime('%Y-%m-%d')}\")\n",
    "\tlog.info(f\"Output format set to {output_format}\")\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    try:\n",
    "        if not filename.startswith(\"._\") and filename.endswith(\".zst\") and \"comments\" in filename:  # Process only zst files\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, os.path.splitext(filename)[0])  # Use output_folder here\n",
    "            process_file(input_file_path, output_file_path,output_format, field, values, from_date, to_date, single_field, exact_match)\n",
    "        else:\n",
    "            continue  # Skip files that are not comments files\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error processing file {filename}: {e}\")\n",
    "        continue  # Move on to the next file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc777fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find zero byte files in subreddits23_csv and save as list \n",
    "import os\n",
    "import csv\n",
    "\n",
    "path = r\"/Volumes/Untitled/reddit/subreddits23_csv\"\n",
    "zero_byte_files = []\n",
    "for filename in os.listdir(path):\n",
    "    if os.path.getsize(os.path.join(path, filename)) == 0:\n",
    "        zero_byte_files.append(filename)\n",
    "zero_byte_files\n",
    "\n",
    "# remove \".csv\" from the end of each file name\n",
    "zero_byte_files = [file[:-4] for file in zero_byte_files]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zero_byte_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cf2df",
   "metadata": {},
   "source": [
    "For these files, I need to go through the terminal to decompress them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing commands to copy paste into terminal \n",
    "for file in zero_byte_files:\n",
    "    print(f\"zstd -d /Volumes/Untitled/reddit/subreddits23/{file}.zst -o {file}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing commands to copy paste into terminal and move files to zerobyte_files folder\n",
    "for file in zero_byte_files:\n",
    "    print(f\"mv ~/{file}.txt /Volumes/Untitled/reddit/zerobyte_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab278d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert zero byte files to CSV\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the directory containing the .txt files and the output directory\n",
    "input_directory = \"/Volumes/Untitled/reddit/zerobyte_files/\"\n",
    "output_directory = \"/Volumes/Untitled/reddit/zerobyte_files_csv/\"\n",
    "\n",
    "# Define the fields to include in the CSV\n",
    "csv_fields = [\n",
    "    \"score\", \n",
    "    \"created_utc\", \n",
    "    \"author\", \n",
    "    \"permalink\", \n",
    "    \"body\", \n",
    "    \"is_submitter\", \n",
    "    \"num_comments\", \n",
    "    \"title\", \n",
    "    \"url\"\n",
    "]\n",
    "\n",
    "def parse_json_line(line):\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "        created = datetime.utcfromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        csv_row = {\n",
    "            \"score\": obj.get(\"score\", \"\"),\n",
    "            \"created_utc\": created,\n",
    "            \"author\": f\"u/{obj.get('author', '')}\",\n",
    "            \"permalink\": f\"https://www.reddit.com{obj.get('permalink', '')}\",\n",
    "            \"body\": obj.get(\"body\", \"\"),\n",
    "            \"is_submitter\": obj.get(\"is_submitter\", \"\"),\n",
    "            \"num_comments\": obj.get(\"num_comments\", \"\"),\n",
    "            \"title\": obj.get(\"title\", \"\"),\n",
    "            \"url\": obj.get(\"url\", \"\")\n",
    "        }\n",
    "        \n",
    "        return csv_row\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def read_and_convert(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='latin-1') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=csv_fields)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for line in infile:\n",
    "            csv_row = parse_json_line(line.strip())\n",
    "            if csv_row:\n",
    "                writer.writerow(csv_row)\n",
    "\n",
    "# Process all .txt files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "        output_file_name = filename.replace(\".txt\", \".csv\")\n",
    "        output_file_path = os.path.join(output_directory, output_file_name)\n",
    "        \n",
    "        print(f\"Processing {input_file_path} to {output_file_path}\")\n",
    "        read_and_convert(input_file_path, output_file_path)\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_path = r\"/Volumes/Untitled/reddit/subreddits23_csv\"\n",
    "output_path = r\"/Volumes/Untitled/reddit/cleaned_subreddits\"\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    file_path = os.path.join(input_path, filename)\n",
    "    \n",
    "    # skip processing if the file is zero bytes (separate cleaning process for these files)\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        continue\n",
    "\n",
    "    if \"comments\" in filename:\n",
    "        column_names = ['score', 'date', 'is_submitter', 'user', 'link', 'body']\n",
    "    else:\n",
    "        # column names for submissions\n",
    "        column_names = ['score', 'date', 'title', 'num_comments', 'user', 'link', 'body']\n",
    "    \n",
    "    # read csv file with specified column names\n",
    "    df = pd.read_csv(file_path, names=column_names, header=None, encoding='ISO-8859-1')\n",
    "    \n",
    "    # check if all columns are present\n",
    "    if not all(col in df.columns for col in column_names):\n",
    "        print(f\"Skipping {filename} due to missing columns\")\n",
    "        continue\n",
    "\n",
    "    if \"comments\" in filename:\n",
    "        # add a column for is_comment\n",
    "        df['is_comment'] = 1\n",
    "    else:\n",
    "        # add a column for is_comment\n",
    "        df['is_comment'] = 0\n",
    "    \n",
    "    # remove rows where 'user' is \"u/[deleted]\"\n",
    "    df = df[~df['user'].isin([\"u/[deleted]\"])]\n",
    "    \n",
    "    # remove rows where 'body' is \"[deleted]\", \"[removed]\", or empty\n",
    "    df = df[~df['body'].isin([\"[deleted]\", \"[removed]\", \"\"])]\n",
    "    \n",
    "    # remove rows where 'body' has only one word\n",
    "    df = df[df['body'].apply(lambda x: len(str(x).split()) > 1)]\n",
    "    \n",
    "    # change 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    # add a column for subreddit\n",
    "    df['subreddit'] = filename.split(\"_\")[0]\n",
    "    \n",
    "    # save cleaned file to cleaned_subreddits folder\n",
    "    cleaned_file_path = os.path.join(output_path, filename)\n",
    "    df.to_csv(cleaned_file_path, index=False)\n",
    "    print(f\"Saved {filename} to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "## now to handle files in zero_bytes folder\n",
    "## they have 0kb in subreddit23_csv folder and were processed separately and moved to zero_bytes folder\n",
    "input_path = r\"/Volumes/Untitled/reddit/zerobyte_files\"\n",
    "output_path = r\"/Volumes/Untitled/reddit/cleaned_subreddits\"\n",
    "csv_fields = [\n",
    "    \"score\", \n",
    "    \"created_utc\", \n",
    "    \"author\", \n",
    "    \"permalink\", \n",
    "    \"body\", \n",
    "    \"is_submitter\", \n",
    "    \"num_comments\", \n",
    "    \"title\", \n",
    "    \"url\"\n",
    "]\n",
    "for filename in os.listdir(input_path):\n",
    "    file_path = os.path.join(input_path, filename)\n",
    "    df = pd.read_csv(file_path, names= csv_fields, encoding='utf-8')\n",
    "    if \"comments\" in filename:\n",
    "        # drop uneccessary columns\n",
    "        df.drop(columns=[\"is_submitter\", \"num_comments\", \"title\", \"url\"], inplace=True)\n",
    "        df.columns = ['score', 'date', 'user', 'link', 'body']\n",
    "    else:\n",
    "        # drop uneccessary columns\n",
    "        df.drop(columns=[\"body\", \"is_submitter\"], inplace=True)\n",
    "        # column names for submissions\n",
    "        df.columns = ['score', 'date', 'user', 'link', 'num_comments', 'title', 'body']\n",
    "    \n",
    "    # add a column for is_comment\n",
    "    if \"comments\" in filename:\n",
    "        # add a column for is_comment\n",
    "        df['is_comment'] = 1\n",
    "    else:\n",
    "        # add a column for is_comment\n",
    "        df['is_comment'] = 0\n",
    "    \n",
    "    # add a column for subreddit\n",
    "    df['subreddit'] = filename.split(\"_\")[0]\n",
    "    \n",
    "    # remove rows where 'user' is \"u/[deleted]\"\n",
    "    df = df[~df['user'].isin([\"u/[deleted]\"])]\n",
    "\n",
    "    # remove rows where 'body' is \"[deleted]\", \"[removed]\", or empty\n",
    "    df = df[~df['body'].isin([\"[deleted]\", \"[removed]\", \"\"])]\n",
    "\n",
    "    # remove rows where 'body' has only one word\n",
    "    df = df[df['body'].apply(lambda x: len(str(x).split()) > 1)]\n",
    "\n",
    "    # change 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    # save cleaned file to cleaned_subreddits folder\n",
    "    cleaned_file_path = os.path.join(output_path, filename)\n",
    "    df.to_csv(cleaned_file_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {filename} to {cleaned_file_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401416c0",
   "metadata": {},
   "source": [
    "## Merging all reddit together for preprocessing whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge all files in immigration_subreddits folder\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_path = r\"/Volumes/Untitled/reddit/cleaned_subreddits\"\n",
    "output_path = r\"/Volumes/Untitled/reddit\"\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the files in the input directory\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    file_path = os.path.join(input_path, filename)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', lineterminator='\\n')\n",
    "    \n",
    "    # Append the data to the merged DataFrame\n",
    "    merged_df = pd.concat([merged_df, df])\n",
    "# Save the merged data to a CSV file\n",
    "merged_df.to_csv(r\"/Volumes/Untitled/reddit/all_reddit_whole.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aede9ea",
   "metadata": {},
   "source": [
    "## Filtering comments about immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\")\n",
    "\n",
    "input_path = r\"/Volumes/Untitled/reddit/cleaned_subreddits1\"\n",
    "output_path = r\"/Volumes/Untitled/reddit/immigration_subreddits\"\n",
    "\n",
    "# Define immigration-related keywords, including terms about \"stop the boat\" and people coming via boats\n",
    "immigration_keywords = (\n",
    "    r'\\bimmigrat\\w*\\b|'\n",
    "    r'\\bRwanda\\s(Bill|Policy)\\b|'\n",
    "    r'\\b(asylum|refugee|asylum-seeker|refugees|asylum-seekers)\\b|'\n",
    "    r'\\bvisa\\w*\\b|'\n",
    "    r'\\b(undocumented|illegal)\\simmigrant\\w*\\b|'\n",
    "    r'\\b(deportation|detain\\w*|detention)\\b|'\n",
    "    r'\\b(border\\scontrol|immigration\\spolicy|migration\\spolicy)\\b|'\n",
    "    r'\\bpoints-based\\ssystem\\b|'\n",
    "    r'\\b(skilled\\sworker\\svisa|student\\svisa)\\b|'\n",
    "    r'\\b(overstay\\w*|overstayer\\w*)\\b|'\n",
    "    r'\\b(work\\svisa|family\\svisa|spouse\\svisa)\\b|'\n",
    "    r'\\b(settlement|permanent\\sresidence|PR\\sstatus)\\b|'\n",
    "    r'\\b(hostile\\senvironment)\\b|'\n",
    "    r'\\b(integration|assimilation|multiculturalism)\\b|'\n",
    "    r'\\b(naturalization|citizenship)\\b|'\n",
    "    r'\\b(migrant\\w*|expat\\w*)\\b|'\n",
    "    r'\\b(foreigner\\w*|foreign\\sworker\\w*)\\b|'\n",
    "    r'\\b(home\\soffice)\\b|'\n",
    "    r'\\b(Windrush)\\b|'\n",
    "    r'\\b(human\\srights|amnesty)\\b|'\n",
    "    r'\\b(brexit\\simmigration)\\b|'\n",
    "    r'\\b(Ukraine\\srefugee\\w*|Syrian\\srefugee\\w*|Afghan\\srefugee\\w*|Palestinian\\srefugee\\w*|Iranian\\srefugee\\w*|Sudanese\\srefugee\\w*)\\b|'\n",
    "    r'\\b(sponsor\\w*\\svisa)\\b|'\n",
    "    r'\\b(asylum\\sclaim)\\b|'\n",
    "    r'\\b(resettlement\\sscheme|community\\ssponsorship)\\b|'\n",
    "    r'\\b(legal\\simmigration|illegal\\simmigration)\\b|'\n",
    "    r'\\b(immigration\\scontrol)\\b|'\n",
    "    r'\\b(stop\\sthe\\sboats?)\\b|'\n",
    "    r'\\b(boat\\smigrants?)\\b|'\n",
    "    r'\\b(small\\sboats?)\\b|'\n",
    "    r'\\b(channel\\scrossings?)\\b|'\n",
    "    r'\\b(illegal\\sboat\\smigration)\\b|'\n",
    "    r'\\b(intercepting\\sboats?)\\b|'\n",
    "    r'\\b(migrant\\sboats?)\\b|'\n",
    "    r'\\b(people\\ssmugglers?)\\b|'\n",
    "    r'\\b(record\\shigh\\simmigration)\\b|'\n",
    "    r'\\b(surge\\sin\\simmigration)\\b|'\n",
    "    r'\\b(spike\\sin\\simmigration)\\b|'\n",
    "    r'\\b(increase\\sin\\simmigration)\\b|'\n",
    "    r'\\b(rising\\simmigration)\\b|'\n",
    "    r'\\b(peaks?\\sin\\simmigration)\\b|'\n",
    "    r'\\b(highest\\slevels?\\sof\\simmigration)\\b|'\n",
    "    r'\\b(growth\\sin\\simmigration)\\b|'\n",
    "    r'\\b(immigration\\sinflux)\\b|'\n",
    "    r'\\b(record\\snumber\\sof\\simmigrants)\\b|'\n",
    "    r'\\b(record\\slevels?\\sof\\simmigration)\\b|'\n",
    "    r'\\b(influx\\sof\\simmigrants?)\\b|'\n",
    "    r'\\b(surge\\sof\\simmigrants?)\\b|'\n",
    "    r'\\b(immigration\\spatterns)\\b|'\n",
    "    r'\\b(immigration\\strends)\\b|'\n",
    "    r'\\b(population\\sgrowth\\sdue\\sto\\simmigration)\\b|'\n",
    "    r'\\b(immigration\\sstatistics)\\b|'\n",
    "    r'\\b(immigration\\sfigures)\\b|'\n",
    "    r'\\b(immigration\\sdata)\\b|'\n",
    "    r'\\b(new\\simmigration\\srecords?)\\b|'\n",
    "    r'\\b(historical\\simmigration\\slevels?)\\b'\n",
    ")\n",
    "\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    file_path = os.path.join(input_path, filename)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, encoding= 'utf-8', lineterminator='\\n')\n",
    "\n",
    "    if \"comments\" in filename:\n",
    "        column_names = ['score', 'date', 'is_submitter', 'user', 'link', 'body', 'is_comment', 'subreddit']\n",
    "    else:\n",
    "        # column names for submissions\n",
    "        column_names = ['score', 'date', 'title', 'num_comments', 'user', 'link', 'body', 'is_comment', 'subreddit']\n",
    "\n",
    "    # Set column names, reorder body to first column\n",
    "    df = df.reindex(columns=column_names)\n",
    "    df = df[['body'] + [col for col in df.columns if col != 'body']]\n",
    "\n",
    "    # Ensure 'body' is a string type (considering potential issues)\n",
    "    if not pd.api.types.is_string_dtype(df['body']):\n",
    "        try:\n",
    "            # Attempt conversion to string, handling potential errors\n",
    "            df['body'] = df['body'].astype(str)\n",
    "        except (ValueError, TypeError):  # Catch specific errors for robustness\n",
    "            # Handle non-convertible values (e.g., log a message or fill with NaNs)\n",
    "            print(f\"Warning: Encountered non-string values in 'body' column for {filename}.\")\n",
    "\n",
    "    # Filter based on immigration keywords\n",
    "    df_filtered = df[df['body'].str.contains(immigration_keywords, case=False, na=False)]\n",
    "\n",
    "    # Print the current file being processed\n",
    "    print(f\"Processing file: {filename}\")\n",
    "\n",
    "    # Save\n",
    "    output_file_path = os.path.join(output_path, filename)\n",
    "    df_filtered.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved {filename} to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "input_path = r\"/Volumes/Untitled/reddit/cleaned_subreddits\"\n",
    "immigration_subreddits_path = r\"/Volumes/Untitled/reddit/immigration_subreddits\"\n",
    "output_path = r\"/Volumes/Untitled/reddit/cleaned_subreddits1\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename not in os.listdir(immigration_subreddits_path):\n",
    "        source_file_path = os.path.join(input_path, filename)\n",
    "        destination_file_path = os.path.join(output_path, filename)\n",
    "        \n",
    "        # Copy the file to the output directory\n",
    "        shutil.copy(source_file_path, destination_file_path)\n",
    "        print(f\"Copied {filename} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge all files in immigration_subreddits folder\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_path = r\"/Volumes/Untitled/reddit/immigration_subreddits\"\n",
    "output_path = r\"/Volumes/Untitled/reddit\"\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the files in the input directory\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    file_path = os.path.join(input_path, filename)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', lineterminator='\\n')\n",
    "    \n",
    "    # Append the data to the merged DataFrame\n",
    "    merged_df = pd.concat([merged_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged data to a CSV file\n",
    "merged_df.to_csv(r\"/Volumes/Untitled/reddit/all_reddit.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
