{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 5\n",
      "We are scraping page: 6\n",
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 5\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(5, 14):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\"https://www.bbc.co.uk/news/topics/c302m85qe1vt?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_immigration = pd.read_csv(\"bbc-uk-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# function to get body of article\n",
    "def get_body(df, retries=3):\n",
    "    bodies = []\n",
    "    for link in df['link']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "                main_content = soup.find_all(\"div\", {\"class\": \"ssrcss-uf6wea-RichTextComponentWrapper ep2nwvo0\"})\n",
    "\n",
    "                # extract text from the text-block\n",
    "                article_text = ' '.join(content.get_text() for content in main_content).strip()\n",
    "                \n",
    "                # append the body text\n",
    "                bodies.append(article_text)\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    bodies.append(\"\")  # append an empty string if all retries fail\n",
    "\n",
    "    # add the bodies list as a new column to the DataFrame\n",
    "    df['body'] = bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_body(bbc_immigration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 4\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(4, 16):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\"https://www.bbc.co.uk/news/topics/cz4pr2gdg1et?page={page_number}\"\n",
    "\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-migration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_migration = pd.read_csv(\"bbc-uk-migration.csv\")\n",
    "\n",
    "df = get_body(bbc_migration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-migration.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "# define our user headers\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "bbc = f\"https://www.bbc.co.uk/news/topics/c1m1wly10gzt?page=1\"\n",
    "\n",
    "# request webpage\n",
    "res = requests.get(bbc, headers=headers)\n",
    "\n",
    "# check status for debugging\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "# this gets list of articles\n",
    "articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "for article in articles:\n",
    "    # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "    article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "    if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "        continue  # Skip this article and move to the next one\n",
    "    \n",
    "    # append article link\n",
    "    article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "    link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "    all_news_links.append(link)\n",
    "\n",
    "    # append headline\n",
    "    headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "    all_headlines.append(headline)\n",
    "\n",
    "    # append date\n",
    "    all_dates.append(article_date)\n",
    "\n",
    "time.sleep(random.randint(1, 3))\n",
    "\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-visas.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_visas = pd.read_csv(\"bbc-uk-visas.csv\")\n",
    "\n",
    "df = get_body(bbc_visas)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-visas.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Refugees and asylum seekers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 9\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(9, 35):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\" https://www.bbc.co.uk/news/topics/cg41ylwvxmdt?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-refugees.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_refugees = pd.read_csv(\"bbc-uk-refugees.csv\")\n",
    "\n",
    "df = get_body(bbc_refugees)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-refugees.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "bbc_refugees = pd.read_csv(\"bbc-uk-refugees.csv\")\n",
    "bbc_visas = pd.read_csv(\"bbc-uk-visas.csv\")\n",
    "bbc_migration = pd.read_csv(\"bbc-uk-migration.csv\")\n",
    "bbc_immigration = pd.read_csv(\"bbc-uk-immigration.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "bbc = pd.concat([bbc_refugees, bbc_visas, bbc_migration, bbc_immigration])\n",
    "\n",
    "# write to csv\n",
    "bbc.to_csv(r\"all-bbc.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r_/kzyzqy3d39ggsy_pf019hz6m0000gp/T/ipykernel_1199/3996936973.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  bbc[\"date\"] = pd.to_datetime(bbc[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "folder_path = f\"/Volumes/Untitled/news/\"\n",
    "bbc = pd.read_csv(folder_path + \"all-bbc.csv\")\n",
    "# remove rows in 'date' not from 2023\n",
    "bbc = bbc[bbc['date'].str.contains(\"23\")]\n",
    "# convert date to datetime\n",
    "bbc[\"date\"] = pd.to_datetime(bbc[\"date\"])\n",
    "# remove duplicates in links\n",
    "bbc = bbc.drop_duplicates(subset=\"link\")\n",
    "# remove rows with empty body\n",
    "bbc = bbc[bbc[\"body\"].notna()]\n",
    "# remove column link\n",
    "bbc = bbc.drop(columns=\"link\")\n",
    "# add column for source\n",
    "bbc[\"news\"] = \"bbc\"\n",
    "# write to csv\n",
    "bbc.to_csv(folder_path + \"all-bbc.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: politics; Tag: uk/immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"XXX\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/politics?tag=uk/immigration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-politics-uk-immigration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: commentisfree; Tag: uk/immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"XXX\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/commentisfree?tag=uk/immigration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-commentisfree-uk-immigration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: uk-news, Tag: global-development/migration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"XXX\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/uk-news?tag=global-development/migration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-uk_news-global_development-migration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: uk-news; Tag: world/refugees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"XXX\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/uk-news?tag=world/refugees&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-uk_news-world-refugees.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "g1 = pd.read_csv(\"guardian-politics-uk-immigration.csv\")\n",
    "g2 = pd.read_csv(\"guardian-commentisfree-uk-immigration.csv\")\n",
    "g3 = pd.read_csv(\"guardian-uk_news-global_development-migration.csv\")\n",
    "g4 = pd.read_csv(\"guardian-uk_news-world-refugees.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "guardian = pd.concat([g1, g2, g3, g4])\n",
    "\n",
    "# write to csv\n",
    "guardian.to_csv(r\"all-guardian.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "guardian = pd.read_csv(folder_path + \"all-guardian.csv\")\n",
    "# convert date to datetime\n",
    "guardian[\"date\"] = pd.to_datetime(guardian[\"date\"])\n",
    "# remove dates not from 2023\n",
    "guardian = guardian[guardian['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "guardian = guardian.drop_duplicates(subset=\"link\")\n",
    "# remove rows with empty body\n",
    "guardian = guardian[guardian[\"body\"].notna()]\n",
    "# remove column link\n",
    "guardian = guardian.drop(columns=\"link\")\n",
    "# add column for source\n",
    "guardian[\"news\"] = \"guardian\"\n",
    "# write to csv\n",
    "guardian.to_csv(folder_path + \"all-guardian.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n",
      "We are scraping page: 4\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 5):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/immigration\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/immigration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_immigration = pd.read_csv(\"mirror-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# function to get body of article\n",
    "# function to get body of article\n",
    "def get_info(df, retries=3):\n",
    "    all_headlines = []\n",
    "    all_bodies = []\n",
    "    all_dates = []\n",
    "    for link in df['link']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "\n",
    "                # extract the headline\n",
    "                headline_tag = soup.find(\"h1\", class_=\"lead-content__title\")\n",
    "                if headline_tag:\n",
    "                    headline = headline_tag.get_text().strip()\n",
    "                    all_headlines.append(headline)\n",
    "                else:\n",
    "                    all_headlines.append(\"None\")\n",
    "\n",
    "                # extract the date\n",
    "                time_tag = soup.find(\"time\", class_=\"date-published\")\n",
    "                if time_tag:\n",
    "                    date = time_tag.get('datetime')\n",
    "                    all_dates.append(date)\n",
    "                else:\n",
    "                    all_dates.append(\"None\")\n",
    "\n",
    "                # extract the article body\n",
    "                div_element = soup.find('div', class_='article-body')\n",
    "                if div_element:\n",
    "                    paragraphs = div_element.find_all('p')\n",
    "                    article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_headlines.append(\"None\")\n",
    "                    all_dates.append(\"None\")\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "    # add as new column to df\n",
    "    df['headline'] = all_headlines\n",
    "    df['date'] = all_dates\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_info(mirror_immigration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/migration\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/migration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-migration.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_migration = pd.read_csv(\"mirror-migration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_migration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-migration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illegal Immigrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/illegal-immigrants\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/illegal-immigrants?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-illegal-immigrants.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_illegals = pd.read_csv(\"mirror-illegal-immigrants.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_illegals)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-illegal-immigrants.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrant Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n",
      "We are scraping page: 4\n",
      "We are scraping page: 5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 6):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/migrant-crisis\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/migrant-crisis?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-migrant-crisis.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_crisis = pd.read_csv(\"mirror-migrant-crisis.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_crisis)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-migrant-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(7, 21):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/home-office\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/home-office?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-home-office.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_home_office = pd.read_csv(\"mirror-home-office.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_home_office)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-home-office.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Border Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/border-force\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/border-force?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-border-force.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_border_force = pd.read_csv(\"mirror-border-force.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_home_office)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-border-force.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refugee Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/refugee-crisis\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/refugee-crisis?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-refugee-crisis.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_refugee_crisis = pd.read_csv(\"mirror-refugee-crisis.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_refugee_crisis)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-refugee-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asylum-seekers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/asylum-seekers\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/asylum-seekers?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-asylum-seekers.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_asylum = pd.read_csv(\"mirror-asylum-seekers.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_asylum)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-asylum-seekers.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load csvs\n",
    "m1 = pd.read_csv(\"mirror-asylum-seekers.csv\")\n",
    "m2 = pd.read_csv(\"mirror-refugee-crisis.csv\")\n",
    "m3 = pd.read_csv(\"mirror-border-force.csv\")\n",
    "m4 = pd.read_csv(\"mirror-home-office.csv\")\n",
    "m5 = pd.read_csv(\"mirror-migrant-crisis.csv\")\n",
    "m6 = pd.read_csv(\"mirror-illegal-immigrants.csv\")\n",
    "m7 = pd.read_csv(\"mirror-migration.csv\")\n",
    "m8 = pd.read_csv(\"mirror-immigration.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# combine all dataframes\n",
    "mirror = pd.concat([m1, m2, m3, m4, m5, m6, m7, m8])\n",
    "\n",
    "# write to csv\n",
    "mirror.to_csv(r\"all-mirror.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "mirror = pd.read_csv(folder_path + \"all-mirror.csv\")\n",
    "# convert date to datetime\n",
    "mirror[\"date\"] = pd.to_datetime(mirror[\"date\"])\n",
    "# remove dates not from 2023\n",
    "mirror = mirror[mirror['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "mirror = mirror.drop_duplicates(subset=\"link\")\n",
    "# remove rows with link containing \"us-news\"\n",
    "mirror = mirror[~mirror[\"link\"].str.contains(\"us-news\")]\n",
    "# remove rows with empty body\n",
    "mirror = mirror[mirror[\"body\"].notna()]\n",
    "# remove column link\n",
    "mirror = mirror.drop(columns=\"link\")\n",
    "# add column for source\n",
    "mirror[\"news\"] = \"mirror\"\n",
    "# write to csv\n",
    "mirror.to_csv(folder_path + \"all-mirror.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n",
      "We are scraping page: 32\n",
      "We are scraping page: 33\n",
      "We are scraping page: 34\n",
      "We are scraping page: 35\n",
      "We are scraping page: 36\n",
      "We are scraping page: 37\n",
      "We are scraping page: 38\n",
      "We are scraping page: 39\n",
      "We are scraping page: 40\n",
      "We are scraping page: 41\n",
      "We are scraping page: 42\n",
      "We are scraping page: 43\n",
      "We are scraping page: 44\n"
     ]
    }
   ],
   "source": [
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(13, 45):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        express = \"https://www.express.co.uk/latest/immigration\"\n",
    "    else:\n",
    "        express = f\"https://www.express.co.uk/latest/immigration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(express, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"post\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.express.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"express-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload CSV\n",
    "express_immigration = pd.read_csv(\"express-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to get body of article\n",
    "def get_info(df, retries=5):\n",
    "    all_headlines = []\n",
    "    all_dates = []\n",
    "    all_bodies = []\n",
    "    for link in df['link']:  \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "                # Extract the headline information\n",
    "                header_info = soup.find(\"header\", class_=\"clearfix\")\n",
    "\n",
    "                if header_info:\n",
    "                    # Get headline\n",
    "                    headline = header_info.find(\"h1\").get_text().strip()\n",
    "                    all_headlines.append(headline)\n",
    "                else:\n",
    "                    all_headlines.append(\"None\")\n",
    "                \n",
    "                # Extract date information\n",
    "                dates_info = soup.find(\"div\", class_=\"dates\")\n",
    "                if dates_info:\n",
    "                    # Extract the first datetime attribute\n",
    "                    published_time = dates_info.find(\"time\")\n",
    "                    if published_time and published_time.get(\"datetime\"):\n",
    "                        published_date = published_time.get(\"datetime\")\n",
    "                        all_dates.append(published_date)\n",
    "                    else:\n",
    "                        all_dates.append(\"None\")\n",
    "                else:\n",
    "                    all_dates.append(\"None\")\n",
    "                    \n",
    "                # Extract the article body\n",
    "                article_body = soup.find(\"div\", class_=\"text-description\")\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all(\"p\")\n",
    "                    article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "                \n",
    "                # Break the retry loop on success\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_headlines.append(\"None\")\n",
    "                    all_dates.append(\"None\")\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "    # Add as new columns to df\n",
    "    df['headline'] = all_headlines\n",
    "    df['date'] = all_dates\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_info(express_immigration)\n",
    "\n",
    "# Rewrite to CSV\n",
    "df.to_csv(\"express-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrant Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n",
      "We are scraping page: 32\n",
      "We are scraping page: 33\n",
      "We are scraping page: 34\n",
      "We are scraping page: 35\n",
      "We are scraping page: 36\n",
      "We are scraping page: 37\n",
      "We are scraping page: 38\n",
      "We are scraping page: 39\n",
      "We are scraping page: 40\n",
      "We are scraping page: 41\n",
      "We are scraping page: 42\n",
      "We are scraping page: 43\n",
      "We are scraping page: 44\n",
      "We are scraping page: 45\n",
      "We are scraping page: 46\n",
      "We are scraping page: 47\n",
      "We are scraping page: 48\n",
      "We are scraping page: 49\n",
      "We are scraping page: 50\n",
      "We are scraping page: 51\n",
      "We are scraping page: 52\n",
      "We are scraping page: 53\n",
      "We are scraping page: 54\n",
      "We are scraping page: 55\n",
      "We are scraping page: 56\n",
      "We are scraping page: 57\n",
      "We are scraping page: 58\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(12, 59):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        express = \"https://www.express.co.uk/latest/migrant-crisis\"\n",
    "    else:\n",
    "        express = f\"https://www.express.co.uk/latest/migrant-crisis?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(express, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"post\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.express.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"express-migrant-crisis.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload CSV\n",
    "express_migrant_crisis = pd.read_csv(\"express-migrant-crisis.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(express_migrant_crisis)\n",
    "\n",
    "# Rewrite to CSV\n",
    "df.to_csv(\"express-migrant-crisis.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "e1 = pd.read_csv(\"express-immigration.csv\")\n",
    "e2 = pd.read_csv(\"express-migrant-crisis.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "express = pd.concat([e1,e2])\n",
    "\n",
    "# write to csv\n",
    "express.to_csv(r\"all-express.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "express = pd.read_csv(folder_path + \"all-express.csv\")\n",
    "# convert date to datetime\n",
    "express[\"date\"] = pd.to_datetime(express[\"date\"])\n",
    "# remove dates not from 2023\n",
    "express = express[express['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "express = express.drop_duplicates(subset=\"link\")\n",
    "# remove rows with link containing \"/us/\"\n",
    "express = express[~express[\"link\"].str.contains(\"/us/\")]\n",
    "# remove rows with empty body\n",
    "express = express[express[\"body\"].notna()]\n",
    "# remove column link\n",
    "express = express.drop(columns=\"link\")\n",
    "# add column for source\n",
    "express[\"news\"] = \"express\"\n",
    "# write to csv\n",
    "express.to_csv(folder_path + \"all-express.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        daily_mail = \"https://www.dailymail.co.uk/news/immigration/index.html\"\n",
    "    else:\n",
    "        daily_mail = f\"https://www.dailymail.co.uk/news/immigration/index.html?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(daily_mail, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # get main article\n",
    "    main_article = soup.find(\"div\", class_=\"mainArticle--1PqPA article\")\n",
    "    if main_article:\n",
    "        # append headline\n",
    "        headline = main_article.find(\"h2\", class_=\"linkro-darkred\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "        # append article link\n",
    "        link = main_article.find(\"a\").attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "        # append date\n",
    "        date = main_article.find(\"div\", class_=\"firstPubDate--28C6S\").get_text().strip()\n",
    "        all_dates.append(date)\n",
    "    else:\n",
    "        all_headlines.append(\"None\")\n",
    "        all_news_links.append(\"None\")\n",
    "        all_dates.append(\"None\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"article--n-F20\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline = article.find(\"h2\", class_=\"linkro-darkred\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append article link\n",
    "        link = article.find(\"a\").attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date = article.find(\"div\", class_=\"firstPubDate--i6kS2\").get_text().strip()\n",
    "        all_dates.append(date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"daily-mail-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get body of article\n",
    "def get_body(df, retries=3):\n",
    "    all_bodies = []\n",
    "    for link in df['link']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "                # Extract the article body\n",
    "                article_body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all(\"p\")\n",
    "                    article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_bodies.append(\"\")  # append an empty string if all retries fail\n",
    "\n",
    "    # add the bodies list as a new column to the DataFrame\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "daily_mail_immigration = pd.read_csv(\"daily-mail-immigration.csv\")\n",
    "# get body of articles\n",
    "df = get_body(daily_mail_immigration)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"daily-mail-immigration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 2\n",
      "We are scraping page: 3\n",
      "We are scraping page: 4\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(2, 5):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        daily_mail = \"https://www.dailymail.co.uk/news/english-channel/index.html\"\n",
    "    else:\n",
    "        daily_mail = f\"https://www.dailymail.co.uk/news/english-channel/index.html?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(daily_mail, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # get main article\n",
    "    main_article = soup.find(\"div\", class_=\"mainArticle--1PqPA article\")\n",
    "    if main_article:\n",
    "        # append headline\n",
    "        headline = main_article.find(\"h2\", class_=\"linkro-darkred\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "        # append article link\n",
    "        link = main_article.find(\"a\").attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "        # append date\n",
    "        date = main_article.find(\"div\", class_=\"firstPubDate--28C6S\").get_text().strip()\n",
    "        all_dates.append(date)\n",
    "    else:\n",
    "        all_headlines.append(\"None\")\n",
    "        all_news_links.append(\"None\")\n",
    "        all_dates.append(\"None\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"article--n-F20\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline = article.find(\"h2\", class_=\"linkro-darkred\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append article link\n",
    "        link = article.find(\"a\").attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date = article.find(\"div\", class_=\"firstPubDate--i6kS2\").get_text().strip()\n",
    "        all_dates.append(date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"daily-mail-channel.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "daily_mail_channel = pd.read_csv(\"daily-mail-channel.csv\")\n",
    "# get body of articles\n",
    "df = get_body(daily_mail_channel)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"daily-mail-channel.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(7, 16):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        daily_mail = \"https://www.dailymail.co.uk/news/the-home-office/index.html\"\n",
    "    else:\n",
    "        daily_mail = f\"https://www.dailymail.co.uk/news/the-home-office/index.html?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(daily_mail, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # get main article\n",
    "    main_article = soup.find(\"div\", class_=\"mainArticle--1PqPA article\")\n",
    "    if main_article:\n",
    "        # append headline\n",
    "        headline = main_article.find(\"h2\", class_=\"linkro-darkred\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "        # append article link\n",
    "        link = main_article.find(\"a\").attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "        # append date\n",
    "        date = main_article.find(\"div\", class_=\"firstPubDate--28C6S\").get_text().strip()\n",
    "        all_dates.append(date)\n",
    "    else:\n",
    "        all_headlines.append(\"None\")\n",
    "        all_news_links.append(\"None\")\n",
    "        all_dates.append(\"None\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"article--n-F20\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline = article.find(\"h2\", class_=\"linkro-darkred\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append article link\n",
    "        link = article.find(\"a\").attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date = article.find(\"div\", class_=\"firstPubDate--i6kS2\").get_text().strip()\n",
    "        all_dates.append(date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"daily-mail-home-office.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "daily_mail_home = pd.read_csv(\"daily-mail-home-office.csv\")\n",
    "# get body of articles\n",
    "df = get_body(daily_mail_home)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"daily-mail-home-office.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "d1 = pd.read_csv(\"daily-mail-immigration.csv\")\n",
    "d2 = pd.read_csv(\"daily-mail-channel.csv\")\n",
    "d3 = pd.read_csv(\"daily-mail-home-office.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "daily_mail = pd.concat([d1, d2, d3])\n",
    "\n",
    "# write to csv\n",
    "daily_mail.to_csv(r\"all-daily-mail.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r_/kzyzqy3d39ggsy_pf019hz6m0000gp/T/ipykernel_1199/713489808.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  daily_mail[\"date\"] = pd.to_datetime(daily_mail[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "# load csv\n",
    "daily_mail = pd.read_csv(folder_path + \"all-daily-mail.csv\")\n",
    "# convert date to datetime\n",
    "daily_mail[\"date\"] = pd.to_datetime(daily_mail[\"date\"])\n",
    "# remove dates not from 2023\n",
    "daily_mail = daily_mail[daily_mail['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "daily_mail = daily_mail.drop_duplicates(subset=\"link\")\n",
    "# remove rows where body mentions joe biden or america\n",
    "daily_mail = daily_mail[~daily_mail[\"body\"].str.contains(\"Biden|America|USA|Trump|Mexico|Australia\")]\n",
    "# remove rows with empty body\n",
    "daily_mail = daily_mail[daily_mail[\"body\"].notna()]\n",
    "# remove column link\n",
    "daily_mail = daily_mail.drop(columns=\"link\")\n",
    "# add column for source\n",
    "daily_mail[\"news\"] = \"daily-mail\"\n",
    "# write to csv\n",
    "daily_mail.to_csv(folder_path + \"all-daily-mail.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # The Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n",
      "We are scraping page: 32\n",
      "We are scraping page: 33\n",
      "We are scraping page: 34\n",
      "We are scraping page: 35\n",
      "We are scraping page: 36\n",
      "We are scraping page: 37\n",
      "We are scraping page: 38\n",
      "We are scraping page: 39\n",
      "We are scraping page: 40\n",
      "We are scraping page: 41\n",
      "We are scraping page: 42\n",
      "We are scraping page: 43\n",
      "We are scraping page: 44\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(13, 45):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    \n",
    "    sun = f\"https://www.thesun.co.uk/topic/uk-immigration-crisis/page/{page_number}/\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(sun, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"teaser-item teaser__small teaser theme-news\") + soup.find_all(\"div\", class_=\"teaser-item teaser__medium teaser theme-news\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.thesun.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"sun-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get body of article\n",
    "def get_info(df, retries=5):\n",
    "    all_headlines = []\n",
    "    all_dates = []\n",
    "    all_bodies = []\n",
    "    for link in df['link']:  \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, \"html.parser\")\n",
    "\n",
    "                # get headline\n",
    "                header_info = soup.find(\"h1\", class_=\"article__headline\").get_text().strip()\n",
    "                if header_info:\n",
    "                    all_headlines.append(header_info)\n",
    "                else:\n",
    "                    all_headlines.append(\"None\") \n",
    "                \n",
    "                # get date\n",
    "                dates_info = soup.find(\"span\", class_=\"article__timestamp\").get_text().strip()\n",
    "                if dates_info:\n",
    "                    all_dates.append(dates_info)\n",
    "                else:\n",
    "                    all_dates.append(\"None\")\n",
    "                    \n",
    "               # get body\n",
    "                article_body = soup.find(\"div\", class_=\"article__content\")\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all(\"p\")\n",
    "                    article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "                # break the retry loop on success\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_headlines.append(\"None\")\n",
    "                    all_dates.append(\"None\")\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "    # add as new columns to df\n",
    "    df['headline'] = all_headlines\n",
    "    df['date'] = all_dates\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "sun_immigration = pd.read_csv(\"sun-immigration.csv\")\n",
    "# get body of articles\n",
    "df = get_info(sun_immigration)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"sun-immigration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refugee crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(13, 32):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    \n",
    "    sun = f\"https://www.thesun.co.uk/topic/refugee-crisis/page/{page_number}/\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(sun, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"teaser-item teaser__small teaser theme-news\") + soup.find_all(\"div\", class_=\"teaser-item teaser__medium teaser theme-news\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.thesun.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"sun-refugee.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "sun_refugee = pd.read_csv(\"sun-refugee.csv\")\n",
    "# get body of articles\n",
    "df = get_info(sun_refugee)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"sun-refugee.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Border force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        sun = \"https://www.thesun.co.uk/topic/uk-border-force/\"\n",
    "    else:\n",
    "        sun = f\"https://www.thesun.co.uk/topic/uk-border-force/page/{page_number}/\"\n",
    "    # request webpage\n",
    "    res = requests.get(sun, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"teaser-item teaser__small teaser theme-news\") + soup.find_all(\"div\", class_=\"teaser-item teaser__medium teaser theme-news\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.thesun.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"sun-border-force.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "sun_b = pd.read_csv(\"sun-border-force.csv\")\n",
    "# get body of articles\n",
    "df = get_info(sun_b)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"sun-border-force.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 3\n",
      "We are scraping page: 4\n",
      "We are scraping page: 5\n",
      "We are scraping page: 6\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(3, 7):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    sun = f\"https://www.thesun.co.uk/topic/home-office/page/{page_number}/\"\n",
    "    # request webpage\n",
    "    res = requests.get(sun, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"teaser-item teaser__small teaser theme-news\") + soup.find_all(\"div\", class_=\"teaser-item teaser__medium teaser theme-news\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.thesun.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"sun-home-office.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "sun_h = pd.read_csv(\"sun-home-office.csv\")\n",
    "# get body of articles\n",
    "df = get_info(sun_h)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"sun-home-office.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "s1 = pd.read_csv(\"sun-immigration.csv\")\n",
    "s2 = pd.read_csv(\"sun-refugee.csv\")\n",
    "s3 = pd.read_csv(\"sun-border-force.csv\")\n",
    "s4 = pd.read_csv(\"sun-home-office.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "sun = pd.concat([s1, s2, s3, s4])\n",
    "\n",
    "# write to csv\n",
    "sun.to_csv(r\"all-sun.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv\n",
    "sun = pd.read_csv(folder_path + \"all-sun.csv\")\n",
    "# convert date to datetime\n",
    "sun[\"date\"] = pd.to_datetime(sun[\"date\"])\n",
    "# remove dates not from 2023\n",
    "sun = sun[sun['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "sun = sun.drop_duplicates(subset=\"link\")\n",
    "# remove rows with empty body\n",
    "sun = sun[sun[\"body\"].notna()]\n",
    "# remove column link\n",
    "sun = sun.drop(columns=\"link\")\n",
    "# add column for source\n",
    "sun[\"news\"] = \"sun\"\n",
    "# write to csv\n",
    "sun.to_csv(folder_path + \"all-sun.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Telegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n",
      "We are scraping page: 32\n"
     ]
    }
   ],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(11, 33):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    telegraph = f\"https://www.telegraph.co.uk/immigration/page-{page_number}/\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(telegraph, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"card__content\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline_span = article.find('span', class_='u-heading-6 list-headline__text')\n",
    "        if headline_span:\n",
    "            headline = headline_span.find('span').text.strip()\n",
    "            all_headlines.append(headline)\n",
    "        else:\n",
    "            all_headlines.append(\"No headline found\")\n",
    "        \n",
    "        # append article link\n",
    "        href_tag = article.find(\"a\").attrs[\"href\"]\n",
    "        link = \"https://www.telegraph.co.uk\" + href_tag\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date_tag = article.find(\"time\", class_=\"card__date\")\n",
    "        if date_tag:\n",
    "            date = date_tag.get_text().strip()\n",
    "            all_dates.append(date)\n",
    "        else:\n",
    "            all_dates.append(\"No date found\")\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(\"telegraph-immigration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get body of article\n",
    "def get_body(df, retries=3):\n",
    "    all_bodies = []\n",
    "    for link in df['link']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "                # Extract the article body\n",
    "                article_body = soup.find(\"div\", class_=\"articleBodyText section\")\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all(\"p\")\n",
    "                    article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_bodies.append(\"\")  # append an empty string if all retries fail\n",
    "\n",
    "    # add the bodies list as a new column to the DataFrame\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "telegraph_immigration = pd.read_csv(\"telegraph-immigration.csv\")\n",
    "# get body of articles\n",
    "df = get_body(telegraph_immigration)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"telegraph-immigration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(7, 24):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    telegraph = f\"https://www.telegraph.co.uk/migrants/page-{page_number}/\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(telegraph, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"card__content\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline_span = article.find('span', class_='u-heading-6 list-headline__text')\n",
    "        if headline_span:\n",
    "            headline = headline_span.find('span').text.strip()\n",
    "            all_headlines.append(headline)\n",
    "        else:\n",
    "            all_headlines.append(\"No headline found\")\n",
    "        \n",
    "        # append article link\n",
    "        href_tag = article.find(\"a\").attrs[\"href\"]\n",
    "        link = \"https://www.telegraph.co.uk\" + href_tag\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date_tag = article.find(\"time\", class_=\"card__date\")\n",
    "        if date_tag:\n",
    "            date = date_tag.get_text().strip()\n",
    "            all_dates.append(date)\n",
    "        else:\n",
    "            all_dates.append(\"No date found\")\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(\"telegraph-migrants.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "telegraph_migrants = pd.read_csv(\"telegraph-migrants.csv\")\n",
    "# get body of articles\n",
    "df = get_body(telegraph_migrants)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"telegraph-migrants.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 4\n",
      "We are scraping page: 5\n",
      "We are scraping page: 6\n",
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(4, 15):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    telegraph = f\"https://www.telegraph.co.uk/migration/page-{page_number}/\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(telegraph, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"card__content\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline_span = article.find('span', class_='u-heading-6 list-headline__text')\n",
    "        if headline_span:\n",
    "            headline = headline_span.find('span').text.strip()\n",
    "            all_headlines.append(headline)\n",
    "        else:\n",
    "            all_headlines.append(\"No headline found\")\n",
    "        \n",
    "        # append article link\n",
    "        href_tag = article.find(\"a\").attrs[\"href\"]\n",
    "        link = \"https://www.telegraph.co.uk\" + href_tag\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date_tag = article.find(\"time\", class_=\"card__date\")\n",
    "        if date_tag:\n",
    "            date = date_tag.get_text().strip()\n",
    "            all_dates.append(date)\n",
    "        else:\n",
    "            all_dates.append(\"No date found\")\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(\"telegraph-migration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "telegraph_migration = pd.read_csv(\"telegraph-migration.csv\")\n",
    "# get body of articles\n",
    "df = get_body(telegraph_migration)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"telegraph-migration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrant crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 6\n",
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(6, 19):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    telegraph = f\"https://www.telegraph.co.uk/migrant-crisis/page-{page_number}/\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(telegraph, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"card__content\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline_span = article.find('span', class_='u-heading-6 list-headline__text')\n",
    "        if headline_span:\n",
    "            headline = headline_span.find('span').text.strip()\n",
    "            all_headlines.append(headline)\n",
    "        else:\n",
    "            all_headlines.append(\"No headline found\")\n",
    "        \n",
    "        # append article link\n",
    "        href_tag = article.find(\"a\").attrs[\"href\"]\n",
    "        link = \"https://www.telegraph.co.uk\" + href_tag\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date_tag = article.find(\"time\", class_=\"card__date\")\n",
    "        if date_tag:\n",
    "            date = date_tag.get_text().strip()\n",
    "            all_dates.append(date)\n",
    "        else:\n",
    "            all_dates.append(\"No date found\")\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(\"telegraph-migrant-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "telegraph_migration = pd.read_csv(\"telegraph-migrant-crisis.csv\")\n",
    "# get body of articles\n",
    "df = get_body(telegraph_migration)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"telegraph-migrant-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refugee crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    if page_number == 1:\n",
    "        telegraph = \"https://www.telegraph.co.uk/refugee-crisis/\"\n",
    "    else:\n",
    "        telegraph = f\"https://www.telegraph.co.uk/refugee-crisis/page-{page_number}/\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(telegraph, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "    \n",
    "    # get list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"card__content\")\n",
    "\n",
    "    for article in articles:\n",
    "        # append headline\n",
    "        headline_span = article.find('span', class_='u-heading-6 list-headline__text')\n",
    "        if headline_span:\n",
    "            headline = headline_span.find('span').text.strip()\n",
    "            all_headlines.append(headline)\n",
    "        else:\n",
    "            all_headlines.append(\"No headline found\")\n",
    "        \n",
    "        # append article link\n",
    "        href_tag = article.find(\"a\").attrs[\"href\"]\n",
    "        link = \"https://www.telegraph.co.uk\" + href_tag\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append date\n",
    "        date_tag = article.find(\"time\", class_=\"card__date\")\n",
    "        if date_tag:\n",
    "            date = date_tag.get_text().strip()\n",
    "            all_dates.append(date)\n",
    "        else:\n",
    "            all_dates.append(\"No date found\")\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"headline\": all_headlines,\n",
    "        \"link\": all_news_links,\n",
    "        \"date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(\"telegraph-refugee-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "telegraph_refugee = pd.read_csv(\"telegraph-refugee-crisis.csv\")\n",
    "# get body of articles\n",
    "df = get_body(telegraph_refugee)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"telegraph-refugee-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "t1 = pd.read_csv(\"telegraph-immigration.csv\")\n",
    "t2 = pd.read_csv(\"telegraph-migrants.csv\")\n",
    "t3 = pd.read_csv(\"telegraph-migration.csv\")\n",
    "t4 = pd.read_csv(\"telegraph-migrant-crisis.csv\")\n",
    "t5 = pd.read_csv(\"telegraph-refugee-crisis.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "telegraph = pd.concat([t1, t2, t3, t4, t5])\n",
    "\n",
    "# write to csv\n",
    "telegraph.to_csv(r\"all-telegraph.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r_/kzyzqy3d39ggsy_pf019hz6m0000gp/T/ipykernel_1199/876100225.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  telegraph[\"date\"] = pd.to_datetime(telegraph[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "# load csv\n",
    "telegraph = pd.read_csv(folder_path + \"all-telegraph.csv\")\n",
    "# convert date to datetime\n",
    "telegraph[\"date\"] = pd.to_datetime(telegraph[\"date\"])\n",
    "# remove dates not from 2023\n",
    "telegraph = telegraph[telegraph['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "telegraph = telegraph.drop_duplicates(subset=\"link\")\n",
    "# remove rows with \"/books/\" or \"/theatre/\" in link\n",
    "telegraph = telegraph[~telegraph[\"link\"].str.contains(\"/books/|/theatre/|/dance/\")]\n",
    "# remove rows with empty body\n",
    "telegraph = telegraph[telegraph[\"body\"].notna()]\n",
    "# remove rows with mentions of america in body\n",
    "telegraph = telegraph[~telegraph[\"body\"].str.contains(\"Biden|America|USA|Trump|Mexico|New York\")]\n",
    "# remove column link\n",
    "telegraph = telegraph.drop(columns=\"link\")\n",
    "# add column for source\n",
    "telegraph[\"news\"] = \"telegraph\"\n",
    "# write to csv\n",
    "telegraph.to_csv(folder_path + \"all-telegraph.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 3\n",
      "We are scraping page: 4\n",
      "We are scraping page: 5\n",
      "We are scraping page: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(3, 7):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        metro = \"https://metro.co.uk/tag/immigration/\"\n",
    "    else:\n",
    "        metro = f\"https://metro.co.uk/tag/immigration/page/{page_number}/\"\n",
    "    # request webpage\n",
    "    res = requests.get(metro, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # header articles in page 1\n",
    "    li_elements = soup.find_all(\"li\", class_=lambda x: x and 'metro__post' in x.split())\n",
    "\n",
    "    # Extract the href attribute of the <a> tag within each <li>\n",
    "    for li in li_elements:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "            if link:\n",
    "                all_news_links.append(link)\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"metro-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get body of article\n",
    "def get_info(df, retries=5):\n",
    "    all_headlines = []\n",
    "    all_dates = []\n",
    "    all_bodies = []\n",
    "    for link in df['link']:  \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, \"html.parser\")\n",
    "\n",
    "                # get headline\n",
    "                headline = soup.find(\"h1\", class_=\"post-title clear\").get_text().strip()\n",
    "                if headline:\n",
    "                    all_headlines.append(headline)\n",
    "                else:\n",
    "                    all_headlines.append(\"None\") \n",
    "                \n",
    "                # get date\n",
    "                published_info = soup.find(\"span\", class_=\"post-published\").get_text().strip()\n",
    "                if published_info:\n",
    "                    published_date = published_info.replace(\"Published\", \"\").strip()\n",
    "                    all_dates.append(published_date)\n",
    "                else:\n",
    "                    all_dates.append(\"None\")\n",
    "                    \n",
    "                # get body\n",
    "                article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all(\"p\")\n",
    "                    article_text = \"\"\n",
    "                    for paragraph in paragraphs:\n",
    "                        text = paragraph.get_text().strip()\n",
    "                        if \"Get in touch\" in text:\n",
    "                            break\n",
    "                        article_text += text + \"\\n\"\n",
    "                    all_bodies.append(article_text.strip())\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "                # break the retry loop on success\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_headlines.append(\"None\")\n",
    "                    all_dates.append(\"None\")\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "    # add as new columns to df\n",
    "    df['headline'] = all_headlines\n",
    "    df['date'] = all_dates\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "metro_immigration = pd.read_csv(\"metro-immigration.csv\")\n",
    "# get info\n",
    "df = get_info(metro_immigration)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"metro-immigration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        metro = \"https://metro.co.uk/tag/immigration-nation/\"\n",
    "    else:\n",
    "        metro = f\"https://metro.co.uk/tag/immigration-nation/page/{page_number}/\"\n",
    "    # request webpage\n",
    "    res = requests.get(metro, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # header articles in page 1\n",
    "    li_elements = soup.find_all(\"li\", class_=lambda x: x and 'metro__post' in x.split())\n",
    "\n",
    "    # Extract the href attribute of the <a> tag within each <li>\n",
    "    for li in li_elements:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "            if link:\n",
    "                all_news_links.append(link)\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"metro-immigration-nation.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "metro_immigration_nation = pd.read_csv(\"metro-immigration-nation.csv\")\n",
    "# get info\n",
    "df = get_info(metro_immigration_nation)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"metro-immigration-nation.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        metro = \"https://metro.co.uk/tag/migrants/\"\n",
    "    else:\n",
    "        metro = f\"https://metro.co.uk/tag/migrants/page/{page_number}/\"\n",
    "    # request webpage\n",
    "    res = requests.get(metro, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # header articles in page 1\n",
    "    li_elements = soup.find_all(\"li\", class_=lambda x: x and 'metro__post' in x.split())\n",
    "\n",
    "    # Extract the href attribute of the <a> tag within each <li>\n",
    "    for li in li_elements:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "            if link:\n",
    "                all_news_links.append(link)\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"metro-migrants.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "metro_migrants = pd.read_csv(\"metro-migrants.csv\")\n",
    "# get info\n",
    "df = get_info(metro_migrants)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"metro-migrants.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asylum and Immigration Tribunal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "metro = \"https://metro.co.uk/tag/asylum-and-immigration-tribunal/\"\n",
    "# request webpage\n",
    "res = requests.get(metro, headers=headers)\n",
    "soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "# header articles in page 1\n",
    "li_elements = soup.find_all(\"li\", class_=lambda x: x and 'metro__post' in x.split())\n",
    "\n",
    "# Extract the href attribute of the <a> tag within each <li>\n",
    "for li in li_elements:\n",
    "    a_tag = li.find('a')\n",
    "    if a_tag:\n",
    "        link = a_tag.get('href')\n",
    "        if link:\n",
    "            all_news_links.append(link)\n",
    "\n",
    "time.sleep(random.randint(1, 3))\n",
    "    \n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"metro-asylum.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "metro_asylum = pd.read_csv(\"metro-asylum.csv\")\n",
    "# get info\n",
    "df = get_info(metro_asylum)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"metro-asylum.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Border agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "metro = \"https://metro.co.uk/tag/border-agency/\"\n",
    "# request webpage\n",
    "res = requests.get(metro, headers=headers)\n",
    "soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "# header articles in page 1\n",
    "li_elements = soup.find_all(\"li\", class_=lambda x: x and 'metro__post' in x.split())\n",
    "\n",
    "# Extract the href attribute of the <a> tag within each <li>\n",
    "for li in li_elements:\n",
    "    a_tag = li.find('a')\n",
    "    if a_tag:\n",
    "        link = a_tag.get('href')\n",
    "        if link:\n",
    "            all_news_links.append(link)\n",
    "\n",
    "time.sleep(random.randint(1, 3))\n",
    "    \n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"metro-border.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "metro_border = pd.read_csv(\"metro-border.csv\")\n",
    "# get info\n",
    "df = get_info(metro_border)\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"metro-border.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the csvs\n",
    "m1 = pd.read_csv(\"metro-immigration.csv\")\n",
    "m2 = pd.read_csv(\"metro-migrants.csv\")\n",
    "m3 = pd.read_csv(\"metro-asylum.csv\")\n",
    "m4 = pd.read_csv(\"metro-border.csv\")\n",
    "m5 = pd.read_csv(\"metro-immigration-nation.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "metro = pd.concat([m1, m2, m3, m4, m5])\n",
    "\n",
    "# write to csv\n",
    "metro.to_csv(r\"all-metro.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "metro = pd.read_csv(folder_path + \"all-metro.csv\")\n",
    "metro['date'] = pd.to_datetime(metro['date'], format='%b %d, %Y, %I:%M%p')\n",
    "# remove dates not from 2023\n",
    "metro = metro[metro['date'].dt.year == 2023]\n",
    "# remove duplicates in links\n",
    "metro = metro.drop_duplicates(subset=\"link\")\n",
    "# remove rows with empty body\n",
    "metro = metro[metro[\"body\"].notna()]\n",
    "# remove column link\n",
    "metro = metro.drop(columns=\"link\")\n",
    "# add column for source\n",
    "metro[\"news\"] = \"metro\"\n",
    "# write to csv\n",
    "metro.to_csv(folder_path + \"all-metro.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Cleaning From Lexis Nexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Search Terms: </b>\n",
    "\n",
    "(\"UK\" OR \"United Kingdom\" OR \"Britain\" OR \"British\") AND (\"immigration\" OR \"immigrants\" OR \"migrants\" OR \"asylum seekers\" OR \"refugees\" OR \"Rwanda plan\" OR \"Rwanda bill\" OR \"stop the boats\" OR \"visa\" OR \"graduate visa\" OR \"skilled worker visa\" OR \"border control\" OR \"immigration policy\" OR \"deportation\" OR \"immigration law\" OR \"immigration reform\" OR \"immigration crisis\" OR \"immigration debate\" OR \"migration\" OR \"immigration system\" OR \"immigration rules\" OR \"immigration enforcement\" OR \"refugee status\" OR \"asylum policy\")\n",
    "\n",
    "<b> Newspapers used: </b>\n",
    "\n",
    "1. times.co.uk\n",
    "2. the independent \n",
    "\n",
    "<b> Timeline: </b>\n",
    "1 Jan 2023 - 31 Dec 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "#!pip install striprtf\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# get current working directory to merge folder path\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Function to process a single file and extract the article body and date\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as infile:\n",
    "        content = infile.read()\n",
    "        text = rtf_to_text(content)\n",
    "        \n",
    "        # extract content between 'Body' and 'Load-Date' which is the article body\n",
    "        body_match = re.search(r'Body(.*)Load-Date:', text, re.DOTALL)\n",
    "        load_date_match = re.search(r'Load-Date:\\s*(.*)', text)\n",
    "\n",
    "        if body_match:\n",
    "            body = body_match.group(1).strip()\n",
    "        else:\n",
    "            body = \"\"\n",
    "        \n",
    "        if load_date_match:\n",
    "            date = load_date_match.group(1).strip()\n",
    "        else:\n",
    "            date = \"\"\n",
    "\n",
    "        body = body.replace('\\n', ' ')\n",
    "        body = re.sub(r'[^a-zA-Z0-9\\s]', '', body)\n",
    "\n",
    "    return body, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "# path to the folder containing times files\n",
    "folder_path = os.path.join(current_dir, 'times')\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.RTF'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        cleaned_text, date = process_file(file_path)\n",
    "        file_name = os.path.splitext(filename)[0]\n",
    "        times.append({'headline': file_name, 'body': cleaned_text, 'date': date})\n",
    "\n",
    "# create pandas dataframe\n",
    "times_df = pd.DataFrame(times)\n",
    "times_df.to_csv('times.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times1 = []\n",
    "\n",
    "# path to the folder containing times1 files\n",
    "folder_path = os.path.join(current_dir, 'times1')\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.RTF'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        cleaned_text, date = process_file(file_path)\n",
    "        file_name = os.path.splitext(filename)[0]\n",
    "        times.append({'headline': file_name, 'body': cleaned_text, 'date': date})\n",
    "\n",
    "# create pandas dataframe\n",
    "times1_df = pd.DataFrame(times)\n",
    "times1_df.to_csv('times1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the csv\n",
    "times = pd.read_csv('times.csv')\n",
    "times1 = pd.read_csv('times1.csv')\n",
    "\n",
    "# merge the two dataframes\n",
    "times = pd.concat([times, times1], ignore_index=True)\n",
    "times.to_csv('all-times.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "times = pd.read_csv(folder_path + \"all-times.csv\")\n",
    "# convert date to datetime\n",
    "times[\"date\"] = pd.to_datetime(times[\"date\"])\n",
    "# remove rows with empty body, time or headline\n",
    "times = times[times[\"body\"].notna()]\n",
    "times = times[times[\"date\"].notna()]\n",
    "times = times[times[\"headline\"].notna()]\n",
    "# add column for source\n",
    "times[\"news\"] = \"times\"\n",
    "# write to csv\n",
    "times.to_csv(folder_path + \"all-times.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent = []\n",
    "\n",
    "# path to the folder containing times1 files\n",
    "folder_path = os.path.join(current_dir, 'independent')\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.RTF'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        cleaned_text, date = process_file(file_path)\n",
    "        file_name = os.path.splitext(filename)[0]\n",
    "        independent.append({'headline': file_name, 'body': cleaned_text, 'date': date})\n",
    "\n",
    "# create pandas dataframe\n",
    "independent_df = pd.DataFrame(independent)\n",
    "independent_df.to_csv('independent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent1 = []\n",
    "\n",
    "# path to the folder containing times1 files\n",
    "folder_path = os.path.join(current_dir, 'independent1')\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.RTF'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        cleaned_text, date = process_file(file_path)\n",
    "        file_name = os.path.splitext(filename)[0]\n",
    "        independent1.append({'headline': file_name, 'body': cleaned_text, 'date': date})\n",
    "\n",
    "# create pandas dataframe\n",
    "independent1_df = pd.DataFrame(independent1)\n",
    "independent1_df.to_csv('independent1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the csvs\n",
    "independent = pd.read_csv('independent.csv')\n",
    "independent1 = pd.read_csv('independent1.csv')\n",
    "\n",
    "# merge the two dataframes\n",
    "independent = pd.concat([independent, independent1], ignore_index=True)\n",
    "independent.to_csv('all-independent.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "independent = pd.read_csv(folder_path + \"all-independent.csv\")\n",
    "# convert date to datetime\n",
    "independent[\"date\"] = pd.to_datetime(independent[\"date\"])\n",
    "# remove rows with empty body, time or headline\n",
    "independent = independent[independent[\"body\"].notna()]\n",
    "independent = independent[independent[\"date\"].notna()]\n",
    "independent = independent[independent[\"headline\"].notna()]\n",
    "# add column for source\n",
    "independent[\"news\"] = \"independent\"\n",
    "# write to csv\n",
    "independent.to_csv(folder_path + \"all-independent.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "\n",
    "times = pd.read_csv(folder_path + \"all-times.csv\")\n",
    "independent = pd.read_csv(folder_path + \"all-independent.csv\")\n",
    "guardian = pd.read_csv(folder_path + \"all-guardian.csv\")\n",
    "sun = pd.read_csv(folder_path + \"all-sun.csv\")\n",
    "telegraph = pd.read_csv(folder_path + \"all-telegraph.csv\")\n",
    "metro = pd.read_csv(folder_path + \"all-metro.csv\")\n",
    "bbc = pd.read_csv(folder_path + \"all-bbc.csv\")\n",
    "daily_mail = pd.read_csv(folder_path + \"all-daily-mail.csv\")\n",
    "express = pd.read_csv(folder_path + \"all-express.csv\")\n",
    "mirror = pd.read_csv(folder_path + \"all-mirror.csv\")\n",
    "\n",
    "# merge all dataframes\n",
    "all_news = pd.concat([times, independent, guardian, sun, telegraph, metro, bbc, daily_mail, express, mirror])\n",
    "# write to csv\n",
    "all_news.to_csv(folder_path + \"all-news.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news = pd.read_csv(\"/Volumes/Untitled/news/all-news.csv\")\n",
    "# extract text from body column\n",
    "texts = all_news[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('T5-base')\n",
    "model = AutoModelWithLMHead.from_pretrained('T5-base', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_length = 618.9125\n",
    "min_length = int(0.1*average_length)\n",
    "max_length = int(0.2*average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for text in texts:\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    output = model.generate(inputs, min_length=min_length,max_length=max_length)\n",
    "    summary = tokenizer.decode(output[0])\n",
    "\n",
    "    summaries.append(summary)\n",
    "texts['summary'] = summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove <pad> and </s> tokens from summaries\n",
    "summaries = [summary.replace(\"<pad>\", \"\").replace(\"</s>\", \"\") for summary in summaries]\n",
    "summaries = [summary.strip() for summary in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add as new column to dataframe\n",
    "\n",
    "all_news['summary'] = summaries\n",
    "all_news.to_csv(\"/Volumes/Untitled/news/all-news-summarised.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
