{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBC, The Guardian, The Mirror, The Independent, The Times, The Telegraph, The Sun, The Daily Express, The Daily Mail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 5\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(5, 14):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\"https://www.bbc.co.uk/news/topics/c302m85qe1vt?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "        \"Links\": all_news_links,\n",
    "        \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_immigration = pd.read_csv(\"bbc-uk-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# function to get body of article\n",
    "def get_body(df, retries=3):\n",
    "    bodies = []\n",
    "    for link in df['Links']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "                main_content = soup.find_all(\"div\", {\"class\": \"ssrcss-uf6wea-RichTextComponentWrapper ep2nwvo0\"})\n",
    "\n",
    "                # extract text from the text-block\n",
    "                article_text = ' '.join(content.get_text() for content in main_content).strip()\n",
    "                \n",
    "                # append the body text\n",
    "                bodies.append(article_text)\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    bodies.append(\"\")  # append an empty string if all retries fail\n",
    "\n",
    "    # add the bodies list as a new column to the DataFrame\n",
    "    df['Body'] = bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_body(bbc_immigration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 4\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(4, 16):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\"https://www.bbc.co.uk/news/topics/cz4pr2gdg1et?page={page_number}\"\n",
    "\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "        \"Links\": all_news_links,\n",
    "        \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-migration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_migration = pd.read_csv(\"bbc-uk-migration.csv\")\n",
    "\n",
    "df = get_body(bbc_migration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-migration.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "# define our user headers\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "bbc = f\"https://www.bbc.co.uk/news/topics/c1m1wly10gzt?page=1\"\n",
    "\n",
    "# request webpage\n",
    "res = requests.get(bbc, headers=headers)\n",
    "\n",
    "# check status for debugging\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "# this gets list of articles\n",
    "articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "for article in articles:\n",
    "    # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "    article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "    if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "        continue  # Skip this article and move to the next one\n",
    "    \n",
    "    # append article link\n",
    "    article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "    link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "    all_news_links.append(link)\n",
    "\n",
    "    # append headline\n",
    "    headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "    all_headlines.append(headline)\n",
    "\n",
    "    # append date\n",
    "    all_dates.append(article_date)\n",
    "\n",
    "time.sleep(random.randint(1, 3))\n",
    "\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "    \"Links\": all_news_links,\n",
    "    \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-visas.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_visas = pd.read_csv(\"bbc-uk-visas.csv\")\n",
    "\n",
    "df = get_body(bbc_visas)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-visas.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Refugees and asylum seekers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 9\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(9, 35):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\" https://www.bbc.co.uk/news/topics/cg41ylwvxmdt?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "        \"Links\": all_news_links,\n",
    "        \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-refugees.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_refugees = pd.read_csv(\"bbc-uk-refugees.csv\")\n",
    "\n",
    "df = get_body(bbc_refugees)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-refugees.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "bbc_refugees = pd.read_csv(\"bbc-uk-refugees.csv\")\n",
    "bbc_visas = pd.read_csv(\"bbc-uk-visas.csv\")\n",
    "bbc_migration = pd.read_csv(\"bbc-uk-migration.csv\")\n",
    "bbc_immigration = pd.read_csv(\"bbc-uk-immigration.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "bbc = pd.concat([bbc_refugees, bbc_visas, bbc_migration, bbc_immigration])\n",
    "\n",
    "# write to csv\n",
    "bbc.to_csv(r\"all-bbc.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: politics; Tag: uk/immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/politics?tag=uk/immigration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-politics-uk-immigration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: commentisfree; Tag: uk/immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/commentisfree?tag=uk/immigration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-commentisfree-uk-immigration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: uk-news, Tag: global-development/migration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/uk-news?tag=global-development/migration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-uk_news-global_development-migration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: uk-news; Tag: world/refugees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/uk-news?tag=world/refugees&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-uk_news-world-refugees.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load csvs\n",
    "g1 = pd.read_csv(\"guardian-politics-uk-immigration.csv\")\n",
    "g2 = pd.read_csv(\"guardian-commentisfree-uk-immigration.csv\")\n",
    "g3 = pd.read_csv(\"guardian-uk_news-global_development-migration.csv\")\n",
    "g4 = pd.read_csv(\"guardian-uk_news-world-refugees.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "guardian = pd.concat([g1, g2, g3, g4])\n",
    "\n",
    "# write to csv\n",
    "guardian.to_csv(r\"all-guardian.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n",
      "We are scraping page: 4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 5):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/immigration\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/immigration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_immigration = pd.read_csv(\"mirror-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# function to get body of article\n",
    "# function to get body of article\n",
    "def get_info(df, retries=3):\n",
    "    all_headlines = []\n",
    "    all_bodies = []\n",
    "    all_dates = []\n",
    "    for link in df['link']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "\n",
    "                # extract the headline\n",
    "                headline_tag = soup.find(\"h1\", class_=\"lead-content__title\")\n",
    "                if headline_tag:\n",
    "                    headline = headline_tag.get_text().strip()\n",
    "                    all_headlines.append(headline)\n",
    "                else:\n",
    "                    all_headlines.append(\"None\")\n",
    "\n",
    "                # extract the date\n",
    "                time_tag = soup.find(\"time\", class_=\"date-published\")\n",
    "                if time_tag:\n",
    "                    date = time_tag.get('datetime')\n",
    "                    all_dates.append(date)\n",
    "                else:\n",
    "                    all_dates.append(\"None\")\n",
    "\n",
    "                # extract the article body\n",
    "                div_element = soup.find('div', class_='article-body')\n",
    "                if div_element:\n",
    "                    paragraphs = div_element.find_all('p')\n",
    "                    article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_headlines.append(\"None\")\n",
    "                    all_dates.append(\"None\")\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "    # add as new column to df\n",
    "    df['headline'] = all_headlines\n",
    "    df['date'] = all_dates\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_info(mirror_immigration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/migration\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/migration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-migration.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_migration = pd.read_csv(\"mirror-migration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_migration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-migration.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illegal Immigrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/illegal-immigrants\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/illegal-immigrants?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-illegal-immigrants.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_illegals = pd.read_csv(\"mirror-illegal-immigrants.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_illegals)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-illegal-immigrants.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrant Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n",
      "We are scraping page: 4\n",
      "We are scraping page: 5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 6):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/migrant-crisis\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/migrant-crisis?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-migrant-crisis.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_crisis = pd.read_csv(\"mirror-migrant-crisis.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_crisis)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-migrant-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 7\n",
      "We are scraping page: 8\n",
      "We are scraping page: 9\n",
      "We are scraping page: 10\n",
      "We are scraping page: 11\n",
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(7, 21):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/home-office\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/home-office?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-home-office.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_home_office = pd.read_csv(\"mirror-home-office.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_home_office)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-home-office.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Border Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/border-force\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/border-force?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-border-force.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_border_force = pd.read_csv(\"mirror-border-force.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_home_office)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-border-force.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refugee Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 3):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/refugee-crisis\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/refugee-crisis?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-refugee-crisis.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_refugee_crisis = pd.read_csv(\"mirror-refugee-crisis.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_refugee_crisis)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-refugee-crisis.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asylum-seekers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/asylum-seekers\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/asylum-seekers?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-asylum-seekers.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "mirror_asylum = pd.read_csv(\"mirror-asylum-seekers.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(mirror_asylum)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"mirror-asylum-seekers.csv\", encoding=\"utf-8\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load csvs\n",
    "m1 = pd.read_csv(\"mirror-asylum-seekers.csv\")\n",
    "m2 = pd.read_csv(\"mirror-refugee-crisis.csv\")\n",
    "m3 = pd.read_csv(\"mirror-border-force.csv\")\n",
    "m4 = pd.read_csv(\"mirror-home-office.csv\")\n",
    "m5 = pd.read_csv(\"mirror-migrant-crisis.csv\")\n",
    "m6 = pd.read_csv(\"mirror-illegal-immigrants.csv\")\n",
    "m7 = pd.read_csv(\"mirror-migration.csv\")\n",
    "m8 = pd.read_csv(\"mirror-immigration.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# combine all dataframes\n",
    "mirror = pd.concat([m1, m2, m3, m4, m5, m6, m7, m8])\n",
    "\n",
    "# write to csv\n",
    "mirror.to_csv(r\"all-mirror.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n",
      "We are scraping page: 32\n",
      "We are scraping page: 33\n",
      "We are scraping page: 34\n",
      "We are scraping page: 35\n",
      "We are scraping page: 36\n",
      "We are scraping page: 37\n",
      "We are scraping page: 38\n",
      "We are scraping page: 39\n",
      "We are scraping page: 40\n",
      "We are scraping page: 41\n",
      "We are scraping page: 42\n",
      "We are scraping page: 43\n",
      "We are scraping page: 44\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(13, 45):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        express = \"https://www.express.co.uk/latest/immigration\"\n",
    "    else:\n",
    "        express = f\"https://www.express.co.uk/latest/immigration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(express, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"post\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.express.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"express-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Reload CSV\n",
    "express_immigration = pd.read_csv(\"express-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to get body of article\n",
    "def get_info(df, retries=5):\n",
    "    all_headlines = []\n",
    "    all_dates = []\n",
    "    all_bodies = []\n",
    "    for link in df['link']:  \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "                # Extract the headline information\n",
    "                header_info = soup.find(\"header\", class_=\"clearfix\")\n",
    "\n",
    "                if header_info:\n",
    "                    # Get headline\n",
    "                    headline = header_info.find(\"h1\").get_text().strip()\n",
    "                    all_headlines.append(headline)\n",
    "                else:\n",
    "                    all_headlines.append(\"None\")\n",
    "                \n",
    "                # Extract date information\n",
    "                dates_info = soup.find(\"div\", class_=\"dates\")\n",
    "                if dates_info:\n",
    "                    # Extract the first datetime attribute\n",
    "                    published_time = dates_info.find(\"time\")\n",
    "                    if published_time and published_time.get(\"datetime\"):\n",
    "                        published_date = published_time.get(\"datetime\")\n",
    "                        all_dates.append(published_date)\n",
    "                    else:\n",
    "                        all_dates.append(\"None\")\n",
    "                else:\n",
    "                    all_dates.append(\"None\")\n",
    "                    \n",
    "                # Extract the article body\n",
    "                article_body = soup.find(\"div\", class_=\"text-description\")\n",
    "                if article_body:\n",
    "                    paragraphs = article_body.find_all(\"p\")\n",
    "                    article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "                    all_bodies.append(article_text)\n",
    "                else:\n",
    "                    all_bodies.append(\"None\")\n",
    "                \n",
    "                # Break the retry loop on success\n",
    "                break\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    all_headlines.append(\"None\")\n",
    "                    all_dates.append(\"None\")\n",
    "                    all_bodies.append(\"None\")\n",
    "\n",
    "    # Add as new columns to df\n",
    "    df['headline'] = all_headlines\n",
    "    df['date'] = all_dates\n",
    "    df['body'] = all_bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_info(express_immigration)\n",
    "\n",
    "# Rewrite to CSV\n",
    "df.to_csv(\"express-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migrant Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 12\n",
      "We are scraping page: 13\n",
      "We are scraping page: 14\n",
      "We are scraping page: 15\n",
      "We are scraping page: 16\n",
      "We are scraping page: 17\n",
      "We are scraping page: 18\n",
      "We are scraping page: 19\n",
      "We are scraping page: 20\n",
      "We are scraping page: 21\n",
      "We are scraping page: 22\n",
      "We are scraping page: 23\n",
      "We are scraping page: 24\n",
      "We are scraping page: 25\n",
      "We are scraping page: 26\n",
      "We are scraping page: 27\n",
      "We are scraping page: 28\n",
      "We are scraping page: 29\n",
      "We are scraping page: 30\n",
      "We are scraping page: 31\n",
      "We are scraping page: 32\n",
      "We are scraping page: 33\n",
      "We are scraping page: 34\n",
      "We are scraping page: 35\n",
      "We are scraping page: 36\n",
      "We are scraping page: 37\n",
      "We are scraping page: 38\n",
      "We are scraping page: 39\n",
      "We are scraping page: 40\n",
      "We are scraping page: 41\n",
      "We are scraping page: 42\n",
      "We are scraping page: 43\n",
      "We are scraping page: 44\n",
      "We are scraping page: 45\n",
      "We are scraping page: 46\n",
      "We are scraping page: 47\n",
      "We are scraping page: 48\n",
      "We are scraping page: 49\n",
      "We are scraping page: 50\n",
      "We are scraping page: 51\n",
      "We are scraping page: 52\n",
      "We are scraping page: 53\n",
      "We are scraping page: 54\n",
      "We are scraping page: 55\n",
      "We are scraping page: 56\n",
      "We are scraping page: 57\n",
      "We are scraping page: 58\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(12, 59):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        express = \"https://www.express.co.uk/latest/migrant-crisis\"\n",
    "    else:\n",
    "        express = f\"https://www.express.co.uk/latest/migrant-crisis?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(express, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"li\", class_=\"post\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a')\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            href_tag = a_tag.get('href')\n",
    "            link = \"https://www.express.co.uk\" + href_tag\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"express-migrant-crisis.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Reload CSV\n",
    "express_migrant_crisis = pd.read_csv(\"express-migrant-crisis.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "df = get_info(express_migrant_crisis)\n",
    "\n",
    "# Rewrite to CSV\n",
    "df.to_csv(\"express-migrant-crisis.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load csvs\n",
    "e1 = pd.read_csv(\"express-immigration.csv\")\n",
    "e2 = pd.read_csv(\"express-migrant-crisis.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "express = pd.concat([e1,e2])\n",
    "\n",
    "# write to csv\n",
    "express.to_csv(r\"all-express.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
