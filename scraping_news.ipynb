{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBC, The Guardian, The Mirror, The Independent, The Times, The Telegraph, The Sun, The Daily Express, The Daily Mail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 5\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(5, 14):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\"https://www.bbc.co.uk/news/topics/c302m85qe1vt?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "        \"Links\": all_news_links,\n",
    "        \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_immigration = pd.read_csv(\"bbc-uk-immigration.csv\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# function to get body of article\n",
    "def get_body(df, retries=3):\n",
    "    bodies = []\n",
    "    for link in df['Links']:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                res = requests.get(link, headers=headers)\n",
    "                res.raise_for_status()  # raise an HTTPError for bad responses\n",
    "                soup = bs(res.content, 'lxml')\n",
    "                main_content = soup.find_all(\"div\", {\"class\": \"ssrcss-uf6wea-RichTextComponentWrapper ep2nwvo0\"})\n",
    "\n",
    "                # extract text from the text-block\n",
    "                article_text = ' '.join(content.get_text() for content in main_content).strip()\n",
    "                \n",
    "                # append the body text\n",
    "                bodies.append(article_text)\n",
    "                break  # break the retry loop on success\n",
    "            \n",
    "            except (requests.exceptions.RequestException, requests.exceptions.SSLError) as e:\n",
    "                print(f\"Error fetching {link}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    wait_time = 2 ** attempt  # exponential backoff\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {link} after {retries} attempts.\")\n",
    "                    bodies.append(\"\")  # append an empty string if all retries fail\n",
    "\n",
    "    # add the bodies list as a new column to the DataFrame\n",
    "    df['Body'] = bodies\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_body(bbc_immigration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-immigration.csv\", encoding=\"utf-8\", header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 4\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(4, 16):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\"https://www.bbc.co.uk/news/topics/cz4pr2gdg1et?page={page_number}\"\n",
    "\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "        \"Links\": all_news_links,\n",
    "        \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-migration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_migration = pd.read_csv(\"bbc-uk-migration.csv\")\n",
    "\n",
    "df = get_body(bbc_migration)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-migration.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "# define our user headers\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "bbc = f\"https://www.bbc.co.uk/news/topics/c1m1wly10gzt?page=1\"\n",
    "\n",
    "# request webpage\n",
    "res = requests.get(bbc, headers=headers)\n",
    "\n",
    "# check status for debugging\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "# this gets list of articles\n",
    "articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "for article in articles:\n",
    "    # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "    article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "    if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "        continue  # Skip this article and move to the next one\n",
    "    \n",
    "    # append article link\n",
    "    article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "    link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "    all_news_links.append(link)\n",
    "\n",
    "    # append headline\n",
    "    headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "    all_headlines.append(headline)\n",
    "\n",
    "    # append date\n",
    "    all_dates.append(article_date)\n",
    "\n",
    "time.sleep(random.randint(1, 3))\n",
    "\n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "    \"Links\": all_news_links,\n",
    "    \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-visas.csv\", encoding=\"utf-8\", header=\"true\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_visas = pd.read_csv(\"bbc-uk-visas.csv\")\n",
    "\n",
    "df = get_body(bbc_visas)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-visas.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC Refugees and asylum seekers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 9\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "all_headlines = []\n",
    "all_dates = []\n",
    "\n",
    "for page_number in range(9, 35):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    bbc = f\" https://www.bbc.co.uk/news/topics/cg41ylwvxmdt?page={page_number}\"\n",
    "    \n",
    "    # request webpage\n",
    "    res = requests.get(bbc, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"div\", class_=\"ssrcss-1ns4t85-PromoSwitchLayoutAtBreakpoints et5qctl0\")\n",
    "\n",
    "    for article in articles:\n",
    "        # check if the article date contains the word \"Video\" or \"Audio\"\n",
    "        article_date = article.find(\"span\", class_=\"visually-hidden ssrcss-1f39n02-VisuallyHidden e16en2lz0\").get_text().strip()\n",
    "        if \"Video\" in article_date or \"Audio\" in article_date:\n",
    "            continue  # Skip this article and move to the next one\n",
    "        \n",
    "        # append article link\n",
    "        article_info = article.find(\"a\", class_=\"ssrcss-1mrs5ns-PromoLink exn3ah91\")\n",
    "        link = \"https://www.bbc.co.uk\" + article_info.attrs[\"href\"]\n",
    "        all_news_links.append(link)\n",
    "\n",
    "        # append headline\n",
    "        headline = article.find(\"p\", class_=\"ssrcss-15dlehh-PromoHeadline exn3ah96\").get_text().strip()\n",
    "        all_headlines.append(headline)\n",
    "\n",
    "        # append date\n",
    "        all_dates.append(article_date)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"Headline\": all_headlines,\n",
    "        \"Links\": all_news_links,\n",
    "        \"Date\": all_dates\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# write to csv\n",
    "df.to_csv(r\"bbc-uk-refugees.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload csv\n",
    "bbc_refugees = pd.read_csv(\"bbc-uk-refugees.csv\")\n",
    "\n",
    "df = get_body(bbc_refugees)\n",
    "\n",
    "# rewrite to CSV\n",
    "df.to_csv(r\"bbc-uk-refugees.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "bbc_refugees = pd.read_csv(\"bbc-uk-refugees.csv\")\n",
    "bbc_visas = pd.read_csv(\"bbc-uk-visas.csv\")\n",
    "bbc_migration = pd.read_csv(\"bbc-uk-migration.csv\")\n",
    "bbc_immigration = pd.read_csv(\"bbc-uk-immigration.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "bbc = pd.concat([bbc_refugees, bbc_visas, bbc_migration, bbc_immigration])\n",
    "\n",
    "# write to csv\n",
    "bbc.to_csv(r\"all-bbc.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: politics; Tag: uk/immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/politics?tag=uk/immigration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-politics-uk-immigration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: commentisfree; Tag: uk/immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/commentisfree?tag=uk/immigration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-commentisfree-uk-immigration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: uk-news, Tag: global-development/migration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/uk-news?tag=global-development/migration&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-uk_news-global_development-migration.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: uk-news; Tag: world/refugees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"response\":{\"status\":\"error\",\"message\":\"requested page is beyond the number of available pages\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "API_KEY = \"794e32b2-456d-4b24-a0e6-570872ae9488\"\n",
    "page = 1\n",
    "articles_per_page = 50  # maximum number of articles per page\n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://content.guardianapis.com/uk-news?tag=world/refugees&api-key={API_KEY}&from-date=2023-01-01&to-date=2023-12-31&type=article&page={page}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_page_articles = data.get('response', {}).get('results', [])\n",
    "        \n",
    "        if not current_page_articles:  # no more articles available\n",
    "            break\n",
    "        \n",
    "        articles.extend(current_page_articles)\n",
    "        page += 1\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        break\n",
    "\n",
    "with open('guardian-uk_news-world-refugees.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['headline', 'link', 'date', 'body']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article['webUrl']\n",
    "        article_response = requests.get(article_url)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            div_element = soup.find('div', class_='article-body-commercial-selector article-body-viewer-selector dcr-fp1ya')\n",
    "            if div_element:\n",
    "                paragraphs = div_element.find_all('p')\n",
    "                article_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "            else:\n",
    "                article_text = \"No content found in the article.\"\n",
    "            \n",
    "            writer.writerow({\n",
    "                'headline': article['webTitle'],\n",
    "                'link': article_url,\n",
    "                'date': article['webPublicationDate'],\n",
    "                'body': article_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Error retrieving article from {article_url}: {article_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load csvs\n",
    "g1 = pd.read_csv(\"guardian-politics-uk-immigration.csv\")\n",
    "g2 = pd.read_csv(\"guardian-commentisfree-uk-immigration.csv\")\n",
    "g3 = pd.read_csv(\"guardian-uk_news-global_development-migration.csv\")\n",
    "g4 = pd.read_csv(\"guardian-uk_news-world-refugees.csv\")\n",
    "\n",
    "# combine all dataframes\n",
    "guardian = pd.concat([g1, g2, g3, g4])\n",
    "\n",
    "# write to csv\n",
    "guardian.to_csv(r\"all-guardian.csv\", encoding=\"utf-8\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are scraping page: 1\n",
      "We are scraping page: 2\n",
      "We are scraping page: 3\n",
      "We are scraping page: 4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# initialise index, this tracks the page number we are on\n",
    "index = 1\n",
    "\n",
    "# create lists to store our data\n",
    "all_news_links = []\n",
    "\n",
    "for page_number in range(1, 5):\n",
    "    print(f\"We are scraping page: {page_number}\")\n",
    "    # define our user headers\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "    if page_number == 1:\n",
    "        mirror = \"https://www.mirror.co.uk/all-about/immigration\"\n",
    "    else:\n",
    "        mirror = f\"https://www.mirror.co.uk/all-about/immigration?pageNumber={page_number}\"\n",
    "\n",
    "    # request webpage\n",
    "    res = requests.get(mirror, headers=headers)\n",
    "    \n",
    "    # check status for debugging\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "    # this gets list of articles\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    # print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "    for article in articles:\n",
    "        # find the <a> tag within the article that contains data-link-tracking\n",
    "        a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "        # extract the href attribute if the <a> tag is found\n",
    "        if a_tag:\n",
    "            link = a_tag.get('href')\n",
    "\n",
    "        all_news_links.append(link)\n",
    "\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "\n",
    "# Convert data to dataframe\n",
    "data = {\"link\": all_news_links}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv(r\"mirror-immigration.csv\", encoding=\"utf-8\", header=\"true\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mirror.co.uk/news/politics/ex-shopkeeper-local-legend-merseyside-32822286\n",
      "https://www.mirror.co.uk/news/politics/britain-cant-address-illegal-migration-32784196\n",
      "https://www.mirror.co.uk/news/politics/keir-starmer-vows-scrap-rwanda-32781412\n",
      "https://www.mirror.co.uk/news/politics/keir-starmer-unveils-plan-replace-32774786\n",
      "https://www.mirror.co.uk/news/politics/cruel-rwanda-scheme-re-opened-32692692\n",
      "https://www.mirror.co.uk/news/politics/home-office-detain-asylum-seekers-32689199\n",
      "https://www.mirror.co.uk/news/politics/dozens-small-boat-live-bbc-32648560\n",
      "https://www.mirror.co.uk/news/uk-news/brexit-policy-tightening-borders-wont-31660930\n",
      "https://www.mirror.co.uk/news/politics/stop-boats-vow-tatters-arrivals-32276845\n",
      "https://www.mirror.co.uk/news/uk-news/one-charged-over-newhaven-swoop-32150331\n",
      "https://www.mirror.co.uk/news/politics/uncle-tory-minister-tom-tugendhat-32108832\n",
      "https://www.mirror.co.uk/news/uk-news/kids-returning-france-school-trip-32108956\n",
      "https://www.mirror.co.uk/news/uk-news/widows-facing-shameless-irrational-3000-32045833\n",
      "https://www.mirror.co.uk/news/politics/uk-population-projected-reach-74-32000586\n",
      "https://www.mirror.co.uk/news/politics/rishi-sunak-wont-say-hell-31884148\n",
      "https://www.mirror.co.uk/news/uk-news/panicking-woman-faces-being-deported-31697976\n",
      "https://www.mirror.co.uk/news/politics/only-way-sort-social-care-31661140\n",
      "https://www.mirror.co.uk/news/politics/how-mp-vote-rwanda-bill-31656452\n",
      "https://www.mirror.co.uk/news/uk-news/im-sick-waking-up-racist-31650686\n",
      "https://www.mirror.co.uk/news/politics/pregnant-girls-who-been-raped-31554018\n",
      "https://www.mirror.co.uk/news/politics/details-secret-40k-immigration-deal-31535209\n",
      "https://www.mirror.co.uk/news/us-news/woman-astonished-stranger-helps-uncover-31449831\n",
      "https://www.mirror.co.uk/news/politics/rishi-sunak-defends-tory-loudmouth-31448745\n",
      "https://www.mirror.co.uk/news/politics/voters-give-brutal-one-word-31448178\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# define the URL for the first page\n",
    "mirror_url = \"https://www.mirror.co.uk/all-about/immigration\"\n",
    "\n",
    "# define user headers\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# request webpage\n",
    "res = requests.get(mirror_url, headers=headers)\n",
    "\n",
    "# check status for debugging\n",
    "res.raise_for_status()\n",
    "\n",
    "# parse the HTML content\n",
    "soup = bs(res.text, \"html.parser\")\n",
    "\n",
    "# this gets list of articles\n",
    "articles = soup.find_all(\"article\")\n",
    "\n",
    "# print out the href attribute of the <a> tag within each article that contains data-link-tracking\n",
    "for article in articles:\n",
    "    # find the <a> tag within the article that contains data-link-tracking\n",
    "    a_tag = article.find('a', {\"data-link-tracking\": True})\n",
    "    \n",
    "    # extract the href attribute if the <a> tag is found\n",
    "    if a_tag:\n",
    "        href = a_tag.get('href')\n",
    "        print(href)\n",
    "    else:\n",
    "        print(\"No link found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
